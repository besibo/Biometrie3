[
["séance-3-comparer-la-moyenne-de-plus-de-2-groupes.html", "4 Séance 3 : comparer la moyenne de plus de 2 groupes 4.1 Packages et données 4.2 L’analyse de variance à un facteur", " 4 Séance 3 : comparer la moyenne de plus de 2 groupes 4.1 Packages et données Avant toute chose, merci de relire l’introduction (section 2) de ce document, et de suivre toutes les étapes qui y sont décrites (création d’un nouveau sous-dossier nommé TP_2 dans votre dossier Biometrie3, changement du répertoire de travail et création d’un nouveau script). Les packages dont vous aurez besoin pour cette séance, et que vous devez donc charger en mémoire, sont les mêmes que pour la section précédente : les packages du tidyverse (Wickham, 2017), qui permettent de manipuler facilement des tableaux de données et de réaliser des graphiques, le package readr (Wickham et al., 2018), pour importer facilement des fichiers .csv au format tibble, le package readxl (Wickham &amp; Bryan, 2019), pour importer facilement des fichiers Excel au format tibble, le package skimr (Waring et al., 2019), qui permet de calculer des résumés de données très informatifs, et le package car (Fox et al., 2019), qui permet d’effectuer le test de comparaison de variances de Levene. Nous utiliserons aussi le package broom (Robinson &amp; Hayes, 2019), qui fait partie du tidyverse mais qu’il faut charger explicitement. La fonction tidy() de ce package nous permettra de “ranger” correctement les résultats de tests dans un tibble : library(tidyverse) library(readr) library(readxl) library(skimr) library(car) library(broom) Enfin, nous aurons besoin du package DescTools (Ken Aho et al., 2019) afin de réaliser un test spécifique de comparaisons multiples. N’oubliez pas de l’installer si nécessaire, avant de le charger en mémoire : library(DescTools) Ces commandes (que vous devez taper dans vos scripts avant de les exécuter dans la console de RStudio) ne devraient pas renvoyer de messages d’erreur puisque vous avez dû les installer pour reproduire les exemples et réaliser les exercices de la séance 1. Si vous rencontrez des problèmes, merci de consulter la section 3.1 de cet ouvrage, ainsi que la section 2.3 du livre de biométrie 2 disponible en ligne. Vous aurez également besoin des jeux de données suivants : Light.csv Insectes.csv 4.2 L’analyse de variance à un facteur 4.2.1 Exploration préalable des données Voyager dans un pays éloigné peut faire souffrir de décallage horaire. Habituellement, la resynchronisation de l’horloge interne circadienne dans le nouveau fuseau horaire est réalisée grâce à la perception de la lumière par les yeux. Ce changement progressif du rythme de notre horloge interne est appelé “décalage de phase”. Ce phénomène a été étudié par 2 chercheurs en 1998 (Campbell &amp; Murphy, 1998), qui ont montré que ce décallage de phase pouvait également être obtenu en exposant des sujets à la lumière, non pas au niveau de leurs yeux, mais au niveau de leur fosse (ou creux) poplitée, c’est-à-dire, derrière les genoux. Cette découverte a été vivement critiquée par certains, et saluée comme une découverte majeure par d’autres. Toutefois, certains aspects du design expérimental de l’étude de 1998 ont été mis en doute en 2002 : il semble en effet que lors de l’exposition du creux poplité, les yeux de certains patients ont été également exposés à de faibles intensités lumineuses. Pour vérifier les trouvailles de Campbell et Murphy, Wright et Czeisler (Wright &amp; Czeisler, 2002) ont ré-examiné ce phénomène. La nouvelle expérience a évalué les rythmes circadiens en mesurant les cycles quotidiens de production de mélatonine chez 22 participants placés au hasard dans 3 groupes. Les patients étaient réveillés en pleine nuit et exposés : Soit à 3 heures de lumière appliquée exclusivement derrière leurs genoux (groupe knee). Soit à 3 heures de lumière appliquée exclusivement à leurs yeux (groupe eyes). Soit à 3 heures d’obscurité totale (groupe control). Le décalage de phase du cycle de production de mélatonine était mesuré 48h plus tard. Des chiffres négatifs indiquent un retard de production de mélatonine. C’est l’effet théorique attendu du traitement lumineux administré. Un décalage de phase positif indique une production de mélatonine plus précoce. Une absence de changement se traduit par un décalage de phase de 0. 4.2.1.1 Importation et examen visuel Les données brutes de cette étude sont fournies dans le fichier Light.csv. Importez ces données dans RStudio et examinez les données brutes grâce à la fonction View(). Light ## # A tibble: 22 x 2 ## treatment shift ## &lt;chr&gt; &lt;dbl&gt; ## 1 control 0.53 ## 2 control 0.36 ## 3 control 0.2 ## 4 control -0.37 ## 5 control -0.6 ## 6 control -0.64 ## 7 control -0.68 ## 8 control -1.27 ## 9 knee 0.73 ## 10 knee 0.31 ## # … with 12 more rows Le tableau obtenu est-il au format long ou au format court/large ? Pourquoi un tableau au format suivant n’aurait-il pas de sens ? ## # A tibble: 8 x 3 ## control eyes knee ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.53 -0.78 0.73 ## 2 0.36 -0.86 0.31 ## 3 0.2 -1.35 0.03 ## 4 -0.37 -1.48 -0.290 ## 5 -0.6 -1.52 -0.56 ## 6 -0.64 -2.04 -0.96 ## 7 -0.68 -2.83 -1.61 ## 8 -1.27 NA NA Lorsque l’on réalise une analyse de variance, puisque les effectifs ne sont pas nécessairement identiques dans tous les groupes (c’est ce qu’on appelle un design déséquilibré, ou “unbalanced design”), présenter les tableaux au format long est indispensable. Par ailleurs, notez que les ANOVAs réalisées sur des “balanced design” (ou designs équilibrés, pour lesquels tous les groupes sont de même taille), sont beaucoup plus puissantes que les ANOVAs réalisées sur des “unbalanced designs”. Ici, le tableau de données est très simple (et de petite taille). Il n’y a pas de données manquantes et aucune création de nouvelle variable n’est nécessaire. La seule modification que nous devrions faire est de transformer la variable treatment en facteur : Light &lt;- Light %&gt;% mutate(treatment = factor(treatment)) Comme toujours, les niveaux du facteur sont automatiquement classés par ordre alphabétique : levels(Light$treatment) ## [1] &quot;control&quot; &quot;eyes&quot; &quot;knee&quot; Pour les statistiques descriptives et les graphiques qui viendront après, nous souhaitons indiquer l’ordre suivant : control, puis knee, puis eyes : Light &lt;- Light %&gt;% mutate(treatment = fct_relevel(treatment, &quot;control&quot;, &quot;knee&quot;, &quot;eyes&quot;)) Light ## # A tibble: 22 x 2 ## treatment shift ## &lt;fct&gt; &lt;dbl&gt; ## 1 control 0.53 ## 2 control 0.36 ## 3 control 0.2 ## 4 control -0.37 ## 5 control -0.6 ## 6 control -0.64 ## 7 control -0.68 ## 8 control -1.27 ## 9 knee 0.73 ## 10 knee 0.31 ## # … with 12 more rows Light$treatment ## [1] control control control control control control control control ## [9] knee knee knee knee knee knee knee eyes ## [17] eyes eyes eyes eyes eyes eyes ## Levels: control knee eyes Attention à bien respecter la casse (le respect des majuscules/minuscules est toujours aussi important dans R). 4.2.1.2 Statistiques descriptives Comme toujours, et maintenant que nos données sont au bon format, il est nécessaire d’examiner quelques statistiques descriptives pour chaque catégorie étudiée. Le plus simple est ici d’utiliser la fonction skim du package skimr : Light %&gt;% group_by(treatment) %&gt;% skim() ## ── Data Summary ──────────────────────── ## Values ## Name Piped data ## Number of rows 22 ## Number of columns 2 ## _______________________ ## Column type frequency: ## numeric 1 ## ________________________ ## Group variables treatment ## ## ── Variable type: numeric ──────────────────────────────────────────────────────────────────────────────────────────── ## skim_variable treatment n_missing complete_rate mean sd p0 p25 ## 1 shift control 0 1 -0.309 0.618 -1.27 -0.65 ## 2 shift knee 0 1 -0.336 0.791 -1.61 -0.76 ## 3 shift eyes 0 1 -1.55 0.706 -2.83 -1.78 ## p50 p75 p100 hist ## 1 -0.485 0.24 0.53 ▂▇▂▁▇ ## 2 -0.290 0.17 0.73 ▃▃▇▃▇ ## 3 -1.48 -1.10 -0.78 ▂▂▁▇▅ Nous avons ici la confirmation que le design expérimental n’est pas équilibré, puisque le groupe control compte un individu de plus. Il semble que le groupe eyes se comporte un peu différemment des autres groupes. En effet, pour les groupes control et knee, les valeurs observées sont très proches : les moyennes et les médianes sont négatives mais proches de 0. les valeurs observées sont négatives pour certaines, et positives pour d’autres (la colonne p0 contient les minimas et la colonne p100 contient les maximas). En revanche, pour le groupe eyes, les décalages de phase observés sont tous négatifs (le maximum, présenté dans la colonne p100 vaut -0.78) et la moyenne est près de 5 fois plus faible que pour les 2 autres groupes. Les écart-types semblent en revanche très proches dans les 3 groupes (entre 0.6 et 0.8). Enfin, les histogrammes présentés pour chaque groupe semblent très éloignés d’une distribution Normale. C’est logique compte tenu des faibles effectifs dans chaque groupe. Nous verrons plus tard que cela n’a aucune importance puisque les conditions d’application de l’ANOVA portent sur les résidus de l’ANOVA, et pas sur les données brutes. Il semble donc que seul le groupe eyes soit véritablement différent du groupe témoin. Pour le vérifier, nous allons d’abord faire quelques représentations graphiques, puis nous ferons un test d’hypothèses. 4.2.1.3 Exploration graphique Comme toujours, il est indispensable de regarder à quoi ressemblent les données brutes sur un ou des graphiques. Les statistiques descriptives ne racontent en effet pas toujours toute l’histoire. Ici, nous allons superposer les données brutes, sous forme de nuage de points, aux boites à moustaches : Light %&gt;% ggplot(aes(x = treatment, y = shift)) + geom_boxplot(notch = TRUE) + geom_jitter(width = 0.2) ## notch went outside hinges. Try setting notch=FALSE. ## notch went outside hinges. Try setting notch=FALSE. ## notch went outside hinges. Try setting notch=FALSE. Puisqu’il y a peu de données, les intervalles de confiance à 95% sont très larges. Ils dépassent d’ailleurs presque systématiquement les quartiles, ce qui explique l’apparence bizarre des boîtes à moustaches et les messages d’avertissement affichés lors de la création du graphique. Il vaudait donc mieux représenter cette figure sans ces intervalles de confiance. Toutefois, avant de les retirer, on peut constater ici que les IC 95% se chevauchent complètement pour les séries control et knee. En revanche, il n’y a aucun chevauchement de l’IC 95% du groupe eyes avec les 2 autres groupes. On s’attend donc à trouver une différence de moyenne significative entre le groupe eyes d’une part, et les groupes control et knee d’autre part, mais pas de différence de moyenne entre les groupes control et knee. Light %&gt;% ggplot(aes(x = treatment, y = shift, fill = treatment)) + geom_boxplot() + geom_jitter(width = 0.2) + theme_bw() + scale_fill_brewer() + labs(x = &quot;&quot;, y = &quot;Décalage de phase&quot;) + theme(legend.position = &quot;none&quot;) On constate ici visuellement que les 3 séries ont une étendue à peu près similaire, et que le groupe eyes semble se distinguer des 2 autres par des valeurs plus faibles. Enfin, les boîtes contenant 50% des valeurs centrales (donc l’étendue des valeurs entre les premiers et troisièmes quartiles) recouvrent le 0 pour les 2 groupes control et knee, mais par pour eyes. 4.2.2 Le test paramétrique Le test paramétrique permettant de comparer la moyenne de plusieurs populations en une seule étape est l’analyse de variance à un facteur. Contrairement aux tests que nous avons vus jusqu’à maintenant, les conditions d’application de ce test ne seront vérifiées qu’après avoir réalisé l’analyse. En effet, les conditions d’application de l’ANOVA ne se vérifient pas sur les données brutes mais sur les résidus de l’ANOVA. C’est d’ailleurs ce que l’on appelle l’analyse des résidus, ou diagnostique de l’ANOVA. 4.2.2.1 Réalisation du test et interprétation Dans R, l’analyse de variance se fait grâce à la fonction aov() (comme “Analysis Of Variance”). La syntaxe est la même que pour un certain nombre de tests déjà vus dans la section 3 : il faut fournir une formule à la fonction. On place la variable numérique expliquée à gauche du ~, et à droite, la variable qualitative explicative (le facteur). Contrairement aux autres tests réalisés jusqu’ici, les résultats du test devront être sauvegardés dans un objet. Outre les résultats du test, cet objet contiendra également tous les éléments permettant de vérifier si les conditions d’application de l’ANOVA sont réunies ou non. Les hypothèses testées sont les suivantes : H\\(_0\\) : les moyennes de toutes les populations sont égales (\\(\\mu_{\\textrm{control}} = \\mu_{\\textrm{knee}} = \\mu_{\\textrm{eyes}}\\)). H\\(_1\\) : toutes les moyennes ne sont pas égales. Au moins l’une d’entre elles diffère des autres. # Réalisation de l&#39;ANOVA 1 facteur res &lt;- aov(shift ~ treatment, data = Light) # Affichage des résultats res ## Call: ## aov(formula = shift ~ treatment, data = Light) ## ## Terms: ## treatment Residuals ## Sum of Squares 7.224492 9.415345 ## Deg. of Freedom 2 19 ## ## Residual standard error: 0.7039492 ## Estimated effects may be unbalanced L’affichage des résultats bruts ne nous apprend que peu de choses. En revanche, la fonction summary() donne la réponse du test : summary(res) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## treatment 2 7.224 3.612 7.289 0.00447 ** ## Residuals 19 9.415 0.496 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Pour le facteur étudié, on obtient le nombre de degrés de libertés (Df), la somme des carrés (Sum Sq), les carrés moyens (Mean Sq), la statistique du test (F) et la \\(p\\)-value (Pr(&gt;F)). Ici, la \\(p\\)-value est inférieure à \\(\\alpha\\), donc on rejette H\\(_0\\). Au moins l’une des moyennes est différente des autres. 4.2.2.2 Conditions d’application Les résultats de l’ANOVA ne seront valides que si les conditions d’application sont vérifiées. Comme indiqué plus haut, ces conditions d’application doivent être vérifiées sur les résidus de l’ANOVA, donc nécessairement après avoir réalisé l’analyse. Les résidus de l’ANOVA représentent l’écart entre chaque observation et la moyenne de son groupe. Les résidus doivent : Être indépendants. Être homogènes. Être distribués normalement. L’indépendance des résidus signifie que connaître la valeur d’un résidu ne permet pas de prédire la valeur d’un autre résidu. Si les données ont été collectées correctement (échantillonnage aléatoire simple, indépendance des observations), on considère généralement que cette condition est vérifiée. Les 2 autres conditions d’application se vérifient soit graphiquement, soit avec un test d’hypothèses. L’homogénéité des résidus signifie que les résidus doivent avoir à peu près la même variance pour chacun des groupes comparés. On peut vérifier que cette condition d’application est vérifiée grâce à ce graphique : plot(res, which = 1) Ici, les résidus sont considérés comme homogènes car nous avons à peu près autant de résidus positifs que négatifs et que la ligne rouge est très proche du 0. L’étalement des résidus est à peu près le même de la gauche à la droite du graphique. On pourrait donc faire entrer les résidus dans une boite rectangulaire horizontale centrée sur le 0 et de la même largeur d’un bout à l’autre du graphique. Une autre façon de visualiser ces résidus est d’utiliser le graphique suivant : plot(res, which = 3) Sur ce graphique, ce qui compte principalement, c’est la droite en rouge. Elle est ici presque horizontale, ce qui montre que les résidus de tous les groupes (un groupe à gauche et 2 à droite) ont à peu près même moyenne. Cette condition d’homogénéité des résidus entre les groupes peut également être vérifiée grâce au test de Levene. Pour ce test, les hypothèses seront les suivantes : H\\(_0\\) : les résidus sont homogènes (i.e. identiques dans tous les groupes). H\\(_1\\) : les résidus ne sont pas homogènes (i.e. au moins un groupe présente des résidus dont la variance est différente des autres). leveneTest(res$residuals ~ Light$treatment) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 2 0.1586 0.8545 ## 19 Ici, puisque \\(p &gt; \\alpha\\), on ne peut pas rejeter l’hypothèse nulle. Les résidus sont donc bien homogènes. Reste à vérifier la normalité des résidus. Là encore, on peut faire cela graphiquement ou avec un test de Shapiro-Wilk : plot(res, which = 2) Sur un graphique quantile-quantile comme celui-là, on considère que les observations sont distribuées normalement si les points sont bien alignés sur la droite. Ici, la plupart des points sont très proches de la droite, ce qui laisse penser que les résidus suivent bien la loi Normale. On peut en avoir la confirmation avec le test suivant : shapiro.test(res$residuals) ## ## Shapiro-Wilk normality test ## ## data: res$residuals ## W = 0.95893, p-value = 0.468 Comme pour tous les tests de Shapiro-Wilk, l’hypothèse nulle est la normalité des observations. Ici, puisque \\(p &gt; \\alpha\\), on ne peut pas rejeter H\\(_0\\). Les résidus suivent donc bien la loi Normale. Toutes les conditions d’application de l’ANOVA sont donc vérifiées. Nous avions donc bien le droit de la réaliser et ses résultats sont valides. On pourrait rédiger les résultats de cette analyse ainsi : Une analyse de variance montre que la moyenne des 3 groupes n’est pas identique (\\(F = 7.289\\), \\(p = 0.004\\)). Un test de Levene a permis de vérifier la condition d’homogénéité de la variance des résidus (\\(F = 0.189\\), \\(p = 0.855\\)), et un test de Shapiro-Wilk a confirmé la normalité des résidus (\\(W = 0.959\\), \\(p = 0.468\\)). Dernière chose, il est possible de produire les 3 graphiques ci-dessus (et même un quatrième que nous ne décrirons pas ici), en une seule commande : plot(res) Il faut alors presser la touche Entrée de votre clavier pour afficher successivement les 4 graphiques produits. À l’issue de cette analyse, deux questions restent en suspens : Entre quels groupes les moyennes sont-elles différentes ? Quelle est la magnitude de ces différences ? Pour répondre à ces 2 questions, il nous faut réaliser des tests a posteriori ou tests post-hoc. 4.2.2.3 Tests post-hoc Lorsqu’une ANOVA montre que tous les groupes n’ont pas la même moyenne, il faut en théorie effectuer toutes les comparaisons de moyennes deux à deux. Le problème est que lorsque l’on effectue des comparaisons multiples, les erreurs \\(\\alpha\\) (probabilité de rejeter à tort H\\(_0\\)) de tous les tests s’ajoutent. Ainsi : pour comparer 3 groupes 2 à 2, nous avons besoin de 3 tests. Pour comparer 4 groupes 2 à 2, nous avons besoin de 6 tests. pour comparer 5 groupes 2 à 2, nous avons besoin de 10 tests. pour comparer 6 groupes 2 à 2, nous avons besoin de 15 tests. pour comparer k groupes 2 à 2, nous avons besoin de \\(\\frac{k(k-1)}{2}\\) tests. Ici, puisque pour chaque test, un risque \\(\\alpha\\) de 5% de rejeter à tort l’hypothèse nulle est commis, réaliser 3 tests ferait monter le risque de s’être trompé quelque part à 15%. C’est la raison pour laquelle des tests spéficiques existent. Nous en verrons 2 : le test de comparaisons multiples de Student et le test de Tukey (ou “Honestly Significant Difference Test”). Pour ces tests, des précautions sont prises qui garantissent que le risque \\(\\alpha\\) global est maîtrisé et qu’il reste fixé à 5%, quel que soit le nombre de comparaisons effectuées. Le test de comparaisons multiples de Student est réalisé avec la fonction pairwise.t.test(). En réalité, ici, 3 tests de Student seront réalisés. Les \\(p\\)-values des tests seront simplement modifiées afin que globalement, le risque \\(\\alpha\\) n’augmente pas. Pour chaque test réalisé, les hypothèses nulles et alternatives sont les mêmes que celles décrites à la section 3.4 : H\\(_0\\) : la moyenne des deux populations est égale (\\(\\mu_1 = \\mu_2\\), soit \\(\\mu_1 - \\mu_2\\) = 0). H\\(_1\\) : la moyenne des deux populations est différente (\\(\\mu_1 \\neq \\mu_2\\), soit \\(\\mu_1 - \\mu_2 \\neq 0\\)). # Réalisation du test post_hoc1 &lt;- pairwise.t.test(Light$shift, Light$treatment) # affichage des résultats post_hoc1 ## ## Pairwise comparisons using t tests with pooled SD ## ## data: Light$shift and Light$treatment ## ## control knee ## knee 0.9418 - ## eyes 0.0088 0.0088 ## ## P value adjustment method: holm Seules les \\(p\\)-values de chaque test sont fournies sous la forme d’une demi-matrice. On constate ainsi qu’une seule \\(p\\) value est supérieure à \\(\\alpha = 0.05\\) : celle du test comparant les moyennes des groupes knee et control. Une autre façon de visualiser ces résultats consiste à utiliser la fonction tidy() du package broom que nous avons mis en mémoire un peu plus tôt. Les résultats seront les mêmes. Ils seront simplement rangés dans un tibble : tidy(post_hoc1) ## # A tibble: 3 x 3 ## group1 group2 p.value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 knee control 0.942 ## 2 eyes control 0.00879 ## 3 eyes knee 0.00880 Nous avons donc la confirmation que les moyennes des groupes knee et control ne sont pas significativement différentes l’une de l’autre. En revanche, la moyenne du groupe eyes est différente de celle des 2 autres groupes (\\(p = 0.009\\) pour les 2 tests). Nous avons donc appris des choses nouvelles, mais nous ne savons toujours pas quelle est la magnitude de la différence détectée entre le groupe eyes et les 2 autres. Le test de Tukey HSD nous permet de répondre à cette question : # Réalisation du test de Tukey HSD post_hoc2 &lt;- TukeyHSD(res) # Affichage des résultats post_hoc2 ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = shift ~ treatment, data = Light) ## ## $treatment ## diff lwr upr p adj ## knee-control -0.02696429 -0.9525222 0.8985936 0.9969851 ## eyes-control -1.24267857 -2.1682364 -0.3171207 0.0078656 ## eyes-knee -1.21571429 -2.1716263 -0.2598022 0.0116776 Cette fois, nous obtenons à la fois la \\(p\\)-value des comparaisons 2 à 2, mais nous obtenons aussi l’estimation des différences de moyennes ainsi que l’intervalle de confiance à 95% de ces différences. Là encore, l’utilisation de la fonction tidy() peut rendre les résultats plus lisibles (ou en tous cas, plus faciles à manipuler) : tidy(post_hoc2) ## # A tibble: 3 x 6 ## term comparison estimate conf.low conf.high adj.p.value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 treatment knee-control -0.0270 -0.953 0.899 0.997 ## 2 treatment eyes-control -1.24 -2.17 -0.317 0.00787 ## 3 treatment eyes-knee -1.22 -2.17 -0.260 0.0117 La première ligne de ce tableau nous confirme une absence de différence de moyenne significative entre les groupes knee et control. La différence de moyenne estimée pour ces deux catégories (\\(\\hat{\\mu}_{\\textrm{knee}} - \\hat{\\mu}_{\\textrm{control}}\\)) vaut \\(-0.027\\), avec un intervalle de confiance à 95% pour cette différence qui vaut \\([-0.95 ; 0.90]\\). Cet intervalle, qui rassemble les valeurs les plus probables pour cette différence de moyenne, contient la valeur 0, ce qui confirme qu’il n’y a aucune raison de penser qu’une différence réelle existe. Le faible écart de moyennes observé entre ces 2 groupes est très vraisemblablement le fruit du hasard. En revanche, les lignes 2 et 3 de ce tableau montrent des différences significatives (\\(p = 0.008\\) et \\(p = 0.012\\) pour les comparaisons eyes/control et eyes/knee respectivement). Les différences sont négatives, de l’ordre de -1.2 pour les 2 comparaisons, ce qui traduit des valeurs plus faibles pour eyes que pour les 2 autres groupes. Pour ces 2 comparaisons, les intervalles de confiance à 95% des différences ne contiennent pas le 0, mais exclusivement des valeurs négatives. En utilisant le tableau ci-dessus, nous pouvons synthétiser graphiquement ces résultats : tidy(post_hoc2) %&gt;% ggplot(aes(x = comparison, y = estimate)) + geom_point() + geom_linerange(aes(ymin = conf.low, ymax = conf.high)) + geom_hline(yintercept = 0, linetype = 2) + labs(x = &quot;Comparaison&quot;, y = &quot;Différence de moyennes (et IC 95%)&quot;) + coord_flip() + theme_bw() Nous avons donc bien montré ici que la re-synchronisation de l’horloge interne n’est possible que par le biais de l’exposition des yeux à la lumière, et non du creux poplité. 4.2.3 L’alternative non paramétrique Dans la suite de cette section, nous faisons l’hypothèse, bien que ça ne soit pas le cas, que les conditions d’application de l’ANOVA ne sont pas vérifiées pour notre jeu de données. 4.2.3.1 Réalisation du tests et interprétation Si les conditions d’application de l’ANOVA ne sont pas remplies, il est nécessaire d’utiliser un test non-paramétrique afin de comparer la moyenne de plus de deux groupes à la fois. La particularité de l’ANOVA est sa robustesse vis-à-vis d’un viol modéré de ses coditions d’application. La robustesse est la capacité d’un test à fournir des résultats qui restent valides même si toutes ses conditions d’application ne sont pas remplies. L’ANOVA étant particulièrement robuste, ses résultats resteront valides dans les situations suivantes : Non normalité modérée des résidus. Si les résidus ne suivent pas parfaitement une loi Normale mais qu’ils sont néanmoins grossièrement distribués selon une courbe en cloche, les résultats de l’ANOVA resteront vrais, surtout si les effectifs sont importants. Non homogénéité des résidus. Si les résidus ne sont pas homogènes dans tous les groupes, les résultats de l’ANOVA resteront vrais tant que les échantillons seront grands, approximativement de la même taille dans tous les groupes, et à condition que les écarts de variances entre les groupes ne dépassent pas un facteur 10. Dans tous les autres cas de non respect des conditions de l’ANOVA, par exemple, si la variance des résidus n’est pas homogène et que les groupes sont de petite taille ou de taille différente, ou si les variances diffèrent de plus d’un facteur 10, ou si les résidus s’écartent fortement de la normalité, ou si les deux conditions d’application sont violées (même modérément) en même temps, il faudra alors faire un test non paramétrique. L’alternative non paramétrique à l’ANOVA à un facteur est le test de la somme des rangs de Kruskal-Wallis dont la syntaxe dans R est similaire à celle de l’ANOVA. Comme d’habitude, l’hypothèse nulle concerne l’absence d’effet du facteur étudié : H\\(_0\\) : le type de traitement appliqué n’a pas d’effet sur le décalage de phase. Les médianes sont égales dans tous les groupes (\\(\\textrm{med}_\\textrm{control} = \\textrm{med}_\\textrm{knee} = \\textrm{med}_\\textrm{eyes}\\)). H\\(_1\\) : le type de traitement appliqué a un d’effet sur le décalage de phase. Les médianes ne sont pas toutes égales, au moins l’une d’entre elles diffère des autres. kruskal.test(shift ~ treatment, data = Light) ## ## Kruskal-Wallis rank sum test ## ## data: shift by treatment ## Kruskal-Wallis chi-squared = 9.4231, df = 2, p-value = 0.008991 Ici, la \\(p\\)-value est inférieure à \\(\\alpha\\), on rejette donc H\\(_0\\) : toutes les médianes ne sont pas égales. Comme avec l’ANOVA, il nous faut maintenant déterminer quelles médianes sont différentes et quelle est la magnitude de cette (ou de ces) différence(s). Pour cela, nous devons réaliser un test de comparaisons multiples. 4.2.3.2 Tests post-hoc Comme pour les tests post-hoc de l’ANOVA, nous allons voir ici 2 tests de comparaisons multiples non paramétriques. Le premier est l’équivalent du test de comparaisons multiples de Student : le test de comparaisons multiples de la somme des rangs de Wilcoxon. Le principe est absolument le même que pour le test de comparaisons multiples de Student : toutes les comparaisons 2 à 2 sont effectuées au moyen d’un test de la somme des rangs de Wilcoxon. Les \\(p\\)-values de ces tests sont corrigées afin de garantir que le risque d’erreur \\(\\alpha\\) global soit maintenu constant en dépit de l’augmentation du nombre de tests réalisés. Pour chaque comparaison, les hypothèses sont les suivantes : H\\(_0\\) : la médiane des deux populations est égale. H\\(_1\\) : la médiane des deux populations est différente. # Réalisation du test post_hoc3 &lt;- pairwise.wilcox.test(Light$shift, Light$treatment) # Affichage des résultats post_hoc3 ## ## Pairwise comparisons using Wilcoxon rank sum test ## ## data: Light$shift and Light$treatment ## ## control knee ## knee 0.9551 - ## eyes 0.0037 0.0524 ## ## P value adjustment method: holm # Utilisation de la fonction `tidy` pour afficher les résultats dans un tibble tidy(post_hoc3) ## # A tibble: 3 x 3 ## group1 group2 p.value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 knee control 0.955 ## 2 eyes control 0.00373 ## 3 eyes knee 0.0524 Ici, la \\(p\\)-value du premier test est supérieure à \\(\\alpha = 0.05\\). Il n’y a donc pas de différence entre les médianes du groupe control et du groupe knee. Le traitement lumineux appliqué dans le creux poplité n’a donc aucun effet sur le décalage de phase. La \\(p\\)-value du second test est en revanche inférieure à \\(\\alpha\\). On rejette l’hypothèse nulle pour ce test ce qui confirme que le traitement lumineux appliqué au niveau des yeux a un effet sur le décalage de phase. Reste toutefois à quantifier l’importance de ce décalage de phase par rapport au groupe control. Enfin, la \\(p\\)-value du troisième test est supérieure (tout juste !) à \\(\\alpha\\). La conclusion logique est donc qu’il n’y a pas de différence significative entre les médianes des groupes knee et control. On sait que ce n’est pas le cas puisque nous avons montré plus haut (avec les tests paramétriques), que la différence de moyenne entre ces deux populations était significative. Nous avons ici l’illustration parfaite de la faible puissance des tests non paramétriques : leur capacité à détecter un effet lorsqu’il y en a réellement un est plus faible que celle des tests paramétriques. En outre, les procédures de comparaisons multiples sont très conservatives, et font mécaniquement baisser la puissance des tests pour maintenir constante l’erreur \\(\\alpha\\). Je ne peux donc que vous inciter à la prudence lorsque vous interprétez les résultats d’un test de comparaions multiples (a fortiori un test non paramétrique) pour lequel la \\(p\\)-value obtenue est très proche du seuil \\(\\alpha\\). Comme pour son homologue paramétrique, le test de comparaisons multiples de Wilcoxon nous permet de prendre une décision par rapport à H\\(_0\\), mais il ne nous dit rien de la magnitude des effets mesurés. Pour les connaître, il nous faut réaliser le test de Dunn (le package DescTools doit être chargé) : # Réalisation du test post_hoc4 &lt;- DunnTest(shift ~ treatment, data = Light) # Affichage des résultats post_hoc4 ## ## Dunn&#39;s test of multiple comparisons using rank sums : holm ## ## mean.rank.diff pval ## knee-control -0.4821429 0.8859 ## eyes-control -9.3392857 0.0164 * ## eyes-knee -8.8571429 0.0214 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Avec ces résultats on progresse un peu, car outre les \\(p\\)-values pour chaque comparaison, le test nous fournit une estimation de la différence des rangs moyens. Malheureusement, ces estimations sont souvent difficiles à interpréter (par exemple, quelle est l’unité utilisée ?) et aucun intervalle de confiance n’est fourni. On constate néanmoins que le test de Dunn donne ici des résultats comparables à ceux fournis par les tests paramétriques : le groupe eyes est significativement différent des deux autres. Pour obtenir les intevalles de confiance dont nous avons besoin, nous n’avons pas d’autre choix que des les calculer à l’aide du test de Wilcoxon classique, en réalisant manuellement les tests dont nous avons besoin. Ici, le test à proprement parler ne nous intéresse pas. La seule chose pertinente est la différence de (pseudo-)médiane estimée et son intervalle de confiance : # Comparaisons entre les groupes `knee` et `eyes` (dans cet ordre) Light %&gt;% filter(treatment %in% c(&quot;knee&quot;, &quot;eyes&quot;)) %&gt;% wilcox.test(shift ~ treatment, data = ., conf.int = TRUE) %&gt;% tidy() ## # A tibble: 1 x 7 ## estimate statistic p.value conf.low conf.high method alternative ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1.19 42 0.0262 0.300 2.21 Wilcoxon rank … two.sided Pour le décalage de phase de ces 2 groupes, la différence de médiane estimée vaut donc 1.19, avec un intervalle de confiance à 95% de \\([0.3 ; 2.21]\\). Toutes les valeurs comprises dans cet intervalle de confiance sont strictement positives. Il y a donc très peu de chances pour que la différence de médiane entre ces deux groupes soit nulle. Le test de Dunn ci-dessus, qui montre une différence significative entre ces groupes, est donc confirmé. 4.2.4 Exercices d’application 4.2.4.1 Cardamine pensylvanica En biologie de la conservation, la question de l’existence d’un lien entre la capacité de dispersion des organismes et le maintien durable des populations dans le temps est étudié de près, notamment en raison de l’anthropisation des milieux qui conduit très souvent à la fragmentation des habitats. Cette question a été étudiée par 2 chercheurs (Molofsky &amp; Ferdy, 2005) chez Cardamine pensylvanica, une plante annuelle d’Amérique du Nord qui produit des graines qui sont dispersées de façon explosive. Quatre traitements ont été utilisés pour modifier expérimentalement la dispersion des graines. La distance entre populations contigües a été définie comme suit : Traitement 1 : continu. Les plants sont conservés au contact les uns des autres. Traitement 2 : medium. Les plants sont séparés de 23.2 centimètres. Tratiement 3 : long. Les plants sont séparés de 49.5 centimètres. Traitement 4 : isole. Les plants sont séparés par des panneaux de bois empêchant la dispersion des graines. Ces traitements ont été assignés au hasard à des populations de plantes, et 4 réplicats ont été faits pour chacun d’entre eux. Les résultats de l’expérience sont présentés ci-dessous. Il s’agit du nombre de générations durant lesquelles les plantes ont persisté : continu : 9, 13, 13, 16 medium : 14, 12, 16, 16 long : 13, 9, 10, 11 isole : 13, 8, 8, 8 Saisissez ces données dans R et faites-en l’analyse. Vous tenterez de déterminer si l’éloignement entre les populations de plantes a un impact sur leur capacité de survie. Comme toujours, avant de vos lancer dans les tests, vous prendrez le temps de décrire les données avec des statistiques descriptives et des représentations graphiques. 4.2.4.2 Insecticides L’efficacité de 6 insecticides nommés A, B, C, D, E et F a été testée sur 6 parcelles agricoles. Chaque insecticide de cette liste a été appliquée sur une parcelle agricole choisie au hasard. Deux semaine plus tard, 12 plants ont été collectés dans chaque parcelle agricole et le nombre d’insectes toujours vivants sur chacun d’entre eux a été compté. Les résultats sont présentés dans le fichier Insectes.csv. Importez ces données dans R et faites-en l’analyse. Tous les insecticides ont-ils la même efficacité ? Si la réponse est non, quels sont les insecticides les plus (ou les moins) efficaces. References "],
["séance-4-corrélations-et-régressions.html", "5 Séance 4 : corrélations et régressions 5.1 Packages et données 5.2 Corrélation 5.3 Régression linéaire", " 5 Séance 4 : corrélations et régressions 5.1 Packages et données Comme dans tous les chapitres de cet ouvrage, nous aurons besoin ici du tidyverse et du package skimr. Noubliez pas de les charger en mémoire avant d’aller plus loin. library(tidyverse) library(skimr) Vous aurez également besoin des jeux de données suivants, disponibles sur l’ENT ou directement téléchargeables depuis ce document : birds.csv loups.csv ropetrick.csv plantbiomass.csv hockey.csv 5.2 Corrélation 5.2.1 Principe Lorsque des variables numériques sont associées ont dit qu’elles sont corrélées. Par exemple, la taille du cerveau et la taille du corps sont corrélées positivement parmi les espèces de mammifères. Les espèces de grande taille ont tendance à avoir un cerveau plus grand et les petites espèces ont tendance à avoir un cerveau plus petit. Le coefficient de corrélation est la quantité qui décrit la force et la direction de l’association entre deux variables numériques mesurées sur un échantillon de sujets ou d’unités d’observation. La corrélation reflète la quantité de dispersion dans un nuage de points entre deux variables. Contrairement à la régression linéaire, la corrélation n’ajuste aucune droite à des données et ne permet donc pas de mesurer à quel point le changement d’une variable entraîne un changement rapide ou lent de l’autre variable. Ainsi, sur la figure ci-dessous, le coefficient de corrélation entre X et Y est le même pour les deux graphiques : il vaut 1. Ici, le coefficient de corrélation (noté \\(r\\)) vaut 1 dans les deux cas, car tous les points sont alignés sur une droite. La pente de la droite n’influence en rien la valeur de corrélation. En revanche, la dispersion des points autour d’une droite parfaite a une influence : Plus la dispersion autour d’une droite parfaite sera grande, plus la corrélation sera faible. C’est la raison pour laquelle lorsque l’on parle de “corrélation”, on sous-entend généralement corrélation linéaire. Ainsi, 2 variables peuvent avoir une relation très forte, mais un coefficient de corrélation nul, si leur relation n’est pas linéaire : L’exploration graphique de vos données devrait donc toujours être une priorité. Calculer un coefficient de corrélation nul ou très faible ne signifie par pour autant une absence de relation entre les 2 variables numériques étudiées. Cela peut signifier une relation non linéaire. La solution la plus simple pour distinguer une relation telle que celle du graphique précédent, et une absence de relation telle que celle présentée dans le graphique ci-dessous, est l’examen visuel des données : En bref, le coefficient de corrélation \\(r\\) est compris entre -1 et +1 : Une forte valeur absolue (\\(r\\) proche de -1 ou +1), indique une relation presque linéaire. Une faible valeur absolue indique soit une absence de relation, soit une relation non linéaire (la visualisation graphique permet généralement d’en savoir plus). Une valeur positive indique qu’une augmentation de la première variable est associée à une augmentation de la seconde variable. Une valeur négative indique qu’une augmentation de la première variable est associée à une diminution de la seconde variable. Dans la suite de ce chapitre, nous allons voir comment calculer le coefficient de corrélation entre 2 variables numériques, et puisque nous travaillons avec des échantillons, ce calcul sera nécessairement entaché d’incertitude. Tout comme la moyenne ou la variance d’un échantillon, la corrélation est un paramètre des populations dont nous ne pourrons qu’estimer la valeur. Toute estimation de corrélation devra donc être encadrée par un intervalle d’incertitude, généralement, il s’agit de l’intervalle de confiance à 95% de la corrélation. Enfin, outre l’estimation de la valeur de la corrélation et de son incertitude, nous pourrons aussi faire des tests d’hypothèses au sujet des corrélations que nous estimerons. 5.2.2 Exploration préalable des données 5.2.2.1 Importation et examen visuel Les adultes qui infligent des mauvais traitements à leurs enfants ont souvent été maltraités dans leur enfance. Une telle relation exsite-t-elle également chez d’autres espèces animales, chez qui cette relation pourrait être étudiée plus facilement ? Müller et al. (2011) ont étudié cette possibilité chez le fou de Grant (Sula granti), un oiseau marin colonial vivant entre autres aux Galápagos. Les jeunes laissés au nid sans attention parentale reçoivent fréquemment la visite d’autres oiseaux, qui se comportent souvent de manière agressive à leur encontre. Les chercheurs ont compté le nombre de ces visites dans le nid de 24 poussins dotés d’une bague d’identification individuelle. Ces 24 individus ont ensuite été suivis à l’âge adulte, lorsqu’ils sont à leur tour devenus parents. Les données récoltées par les chercheurs figurent dans le fichier birds.csv. Importez ces données dans R dans un objet noté birds. birds ## # A tibble: 24 x 2 ## nVisitsNestling futureBehavior ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 -0.8 ## 2 7 -0.92 ## 3 15 -0.8 ## 4 4 -0.46 ## 5 11 -0.47 ## 6 14 -0.46 ## 7 23 -0.23 ## 8 14 -0.16 ## 9 9 -0.23 ## 10 5 -0.23 ## # … with 14 more rows La première colonne de ce tableau indique, pour chaque individu suivi, le nombre de visites reçues au nid de la part d’adultes agressifs lorsqu’ils étaient poussins. La seconde colonne indique, pour ces mêmes individus devenus adultes, le nombre de visites agressives effectuées à des nids d’autres poussins. Ce nombre n’est pas dans la même unité que la première variable car il a été corrigé par d’autres variables d’intérêt pour les chercheurs. Il manque à ce tableau une variable indiquant le code des individus. Elle n’est pas indispensable, mais la rajouter est une bonne habitude à prendre pour toujours travailler avec des “données rangées” : birds &lt;- birds %&gt;% mutate(ID = factor(1:24)) birds ## # A tibble: 24 x 3 ## nVisitsNestling futureBehavior ID ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 1 -0.8 1 ## 2 7 -0.92 2 ## 3 15 -0.8 3 ## 4 4 -0.46 4 ## 5 11 -0.47 5 ## 6 14 -0.46 6 ## 7 23 -0.23 7 ## 8 14 -0.16 8 ## 9 9 -0.23 9 ## 10 5 -0.23 10 ## # … with 14 more rows 5.2.2.2 Statistiques descriptives Comme toujours, la première chose à faire est d’examiner quelques statistiques descriptives pour se faire une idée de la forme des données et pour repérer les éventuelles données manquantes ou aberrantes. skim(birds) ## ── Data Summary ──────────────────────── ## Values ## Name birds ## Number of rows 24 ## Number of columns 3 ## _______________________ ## Column type frequency: ## factor 1 ## numeric 2 ## ________________________ ## Group variables None ## ## ── Variable type: factor ───────────────────────────────────────────────────────────────────────────────────────────── ## skim_variable n_missing complete_rate ordered n_unique ## 1 ID 0 1 FALSE 24 ## top_counts ## 1 1: 1, 2: 1, 3: 1, 4: 1 ## ## ── Variable type: numeric ──────────────────────────────────────────────────────────────────────────────────────────── ## skim_variable n_missing complete_rate mean sd p0 p25 p50 ## 1 nVisitsNestling 0 1 13.1 7.21 1 8.75 13 ## 2 futureBehavior 0 1 -0.119 0.374 -0.92 -0.288 -0.1 ## p75 p100 hist ## 1 15.8 31 ▅▇▅▃▁ ## 2 0.182 0.39 ▂▂▃▂▇ Outre le facteur ID que nous venons de créer, nous disposons donc de 2 variables numériques qui ne contiennent pas de données manquantes. La variable nVisitsNestling, qui indique le nombre de visites agressives reçues par les individus suivis lorsqu’ils étaient de jeunes poussins, varie de 1 à 31, pour une moyenne de 13.12, une médiane proche (13) mais un écart-type important. La variable futureBehavior varie de -0.92 à 0.39, avec une moyenne et une médiane proche de 0 (-0.12 et -0.1 respectivement). À ce stade, il est possible de calculer le coefficient de corrélation linéaire entre les 2 variables : birds %&gt;% select(nVisitsNestling, futureBehavior) %&gt;% cor() ## nVisitsNestling futureBehavior ## nVisitsNestling 1.0000000 0.5337225 ## futureBehavior 0.5337225 1.0000000 Le résultat est obtenu sous la forme d’une matrice symétrique : Sur la diagonale, les corrélations valent 1 (le coefficient de corrélation d’une variable avec elle-même vaut toujours 1). En dehors de la diagonale, on trouve le coefficient de corrélation linéaire entre les 2 variables d’intérêt. Ici, il est positif et vaut 0.534, ce qui est une valeur relativement élevée dans le domaine de la biologie ou de l’écologie. 5.2.2.3 Exploration graphique Afin de savoir si la valeur moyenne de \\(r\\) calculée précédemment reflète une relation linéaire mais moyenne, ou une relation qui n’est pas vraiment linéaire, nous devons faire un nuage de points : birds %&gt;% ggplot(aes(x = nVisitsNestling, y = futureBehavior)) + geom_point() + labs(x = &quot;Nombre de visites reçues par le poussin&quot;, y = &quot;Comportement futur&quot;) + theme_bw() On constate ici que la corrélation moyenne obtenue plus haut est due au fait que les points sont assez dispersés, et non au fait que la relation n’est pas linéaire. On peut donc dire que la relation, si elle existe, n’est pas parfaite. Le comportement des individus devenus adultes semble donc en partie lié au nombre de visites agessives qu’ils ont reçues étant jeunes, mais ce n’est certainement pas le seul facteur influençant leur comportement. Un test d’hypothèses devrait nous permettre de déterminer si la corrélation linéaire observée ici est significativement différente de 0 ou non. 5.2.3 Le test paramétrique Comme pour la plupart des grandeurs calculées à partir d’un échantillon, la corrélation \\(r\\) n’est qu’un estimateur de la corrélation qui existe réellement entre ces deux variables dans la population générale. Dans la population générale, la corrélation linéaire est généralement notée \\(\\rho\\). Son estimateur, \\(r\\) est donc souvent noté \\(\\hat{\\rho}\\). Le test d’hypothèses que nous allons faire maintenant permet de vérifier si le coefficient de corrélation \\(\\rho\\) dans la population générale est différent de 0 ou non. Les hypothèses de ce test sont les suivantes : H\\(_0\\) : le coefficient de corrélation entre les deux variables étudiées vaut 0 dans la population générale (\\(\\rho = 0\\)). H\\(_1\\) : le coefficient de corrélation entre les deux variables étudiées est différent de 0 dans la population générale (\\(\\rho \\neq 0\\)). Ce test est réalisé dans R grâce à la fonction cor.test(). 5.2.3.1 Résultats du test et interprétation cor.test(birds$nVisitsNestling, birds$futureBehavior) ## ## Pearson&#39;s product-moment correlation ## ## data: birds$nVisitsNestling and birds$futureBehavior ## t = 2.9603, df = 22, p-value = 0.007229 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.1660840 0.7710999 ## sample estimates: ## cor ## 0.5337225 Le test réalisé ici est le test de corrélation de Pearson. Il s’agit d’un test paramétrique dont les conditions d’application sont expliquées plus bas. Comme pour tous les tests examinés jusqu’ici, les premières lignes des résultats fournissent toutes les informations utiles au sujet du test. Ici, on peut dire : Au seuil \\(\\alpha = 0.05\\), le test de corrélation de Pearson a permis de rejeter l’hypothèse nulle selon laquelle la corrélation entre le nombre de visites agressives au nid des poussins et leur futur comportement agressif sont indépendants (\\(t = 2.96\\), \\(ddl = 22\\), \\(p = 0.007\\)). Ce test prouve donc que \\(\\rho\\) est statistiquement différent de 0. La valeur de 0.53 observée ici n’est pas due au hasard de l’échantillonnage. Comme toujours, les résultats du test que nous avons réalisé ne nous disent rien de la valeur de la corrélation estimée, ni de son incertitude. Il nous faut pour cela examiner les autres lignes fournies par R lorsque nous faisons ce test et qui relèvent de l’estimation (voir section suivante). Dernière chose concernant ce test, nous avons fait ici un test bilatéral comme nous le rappelle cette ligne des résultats : alternative hypothesis: true correlation is not equal to 0 Comme pour les tests de comparaisons de moyennes, il est possible de réaliser un test unilatéral, à condition que cela ait un sens, à condition que nous soyons en mesure d’expliquer le choix de notre hypothèse alternative. La syntaxe est la même que pour les tests de Student ou de Wilcoxon : on utilise l’argument alternative = &quot;less&quot; ou alternative = &quot;greater&quot; au moment de faire le test, selon l’hypothèse que l’on souhaite tester. Ici, si les hypothèses que nous souhaitons tester sont les suivantes : H\\(_0\\) : le coefficient de corrélation entre les deux variables étudiées vaut 0 dans la population générale (\\(\\rho = 0\\)) H\\(_1\\) : le coefficient de corrélation entre les deux variables étudiées est positif dans la population générale (\\(\\rho &gt; 0\\)) On utilise la syntaxe suivante : cor.test(birds$nVisitsNestling, birds$futureBehavior, alternative = &quot;greater&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: birds$nVisitsNestling and birds$futureBehavior ## t = 2.9603, df = 22, p-value = 0.003615 ## alternative hypothesis: true correlation is greater than 0 ## 95 percent confidence interval: ## 0.2320921 1.0000000 ## sample estimates: ## cor ## 0.5337225 Comme pour les autres test unilatéraux, le choix d’une hypothèse alternative aberrante se traduit par une \\(p-\\)value très forte, généralement égale à (ou très proche de) 1. 5.2.3.2 Estimation et intervalle de confiance Revenons à notre test bilatéral. La section “estimation” des résultats de ce test nous indique que la meilleure estimation du coefficient de corrélation linéaire de Pearson dans la population générale vaut \\(\\hat{\\rho} = 0.533\\). C’est la valeur que nous avions calculé à la main avec la fonction cor(). L’intervalle de confiance à 95% de cette valeur estimée est également fourni. La conclusion de cette procédure pourrait donc être formulée de la façon suivante : Au seuil \\(\\alpha = 0.05\\), le test de corrélation de Pearson a permis de rejeter l’hypothèse nulle selon laquelle la corrélation entre le nombre de visites agressives au nid des poussins et leur futur comportement agressif sont indépendants (\\(t = 2.96\\), \\(ddl = 22\\), \\(p = 0.007\\)). La meilleure estimation de ce coefficient de corrélation dans la population générale vaut \\(\\hat{\\rho} = 0.533\\). La vraie valeur dans la population générale a de bonnes chances de se trouver dans l’inervalle [0.17 ; 0.77] (intervalle de confiance à 95%). 5.2.3.3 Conditions d’application Ce test est un test paramétrique. Pour avoir le droit de le réaliser, il nous faut donc vérifier les conditions d’application suivantes : Les individus doivent être indépendants les uns des autres Les mesures effectuées doivent suivre une distribution Normale bivariée Sauf si on a de bonnes raisons de penser le contraire, on considère généralement que si l’échantillonnage a été fait de façon aléatoire, l’indépendance des observations est garantie. La condition de “distribution Normale bivariée” des données est en revanche nouvelle. Elle suppose essentiellement que les 3 critères suivants soient vérifiés : La relation entre les 2 variables doit être linéaire. C’est que nous tentons de vérifier visuellement en réalisant un nuage de points des données. Sur un graphique représentant une variable en fonction de l’autre, le nuage de points doit avoir une forme circulaire ou elliptique. Là encore, une représentation graphique nous permet d’apprécier cette condition. Les 2 variables étudiées doivent suivre une distribution Normale dans la population générale. Avant de faire ce test, il nous faut donc vérifier la Normalité des données pour chacune des 2 variables séparément, à l’aide, par exemple, d’un test de Shapiro-Wilk. Pour résumer, l’examen du nuage de points permet de vérifier les 2 premières conditions et 2 tests de Shapiro permettent de vérifier la troisième. Pour l’examen du nuage de points, les conditions ne seront pas remplies dans les situations suivantes (voir les exemples du graphique ci-dessous) : Le nuage de points a une forme d’entonnoir ou de nœud papillon. Des ouliers sont présents (quelques points fortement éloignés du reste des observations). Une relation non linéaire existe entre les deux variables. Enfin, si l’une, l’autre ou les deux séries de données ne suivent pas la loi Normale, il faudra faire un test non paramétrique. 5.2.4 L’alternative non paramétrique Quand les conditions d’application du test de corrélation de Pearson ne sont pas remplies, il faut faire un test équivalent non paramétrique. Le test utilisé le plus fréquemment dans cette situation est le test du \\(\\rho\\) de Spearman. On l’effectue comme le test de Pearson en précisant simplement un argument supplémentaire : method = &quot;spearman&quot; (sans maujscule) : cor.test(birds$nVisitsNestling, birds$futureBehavior, method = &quot;spearman&quot;) ## Warning in cor.test.default(birds$nVisitsNestling, birds$futureBehavior, : ## Cannot compute exact p-value with ties ## ## Spearman&#39;s rank correlation rho ## ## data: birds$nVisitsNestling and birds$futureBehavior ## S = 1213.5, p-value = 0.01976 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.472374 Le test de Spearman est au test de Pearson ce que le test de Wilcoxon est au test de Student. Il travaille non pas sur les données brutes (ici, les mesures des scientifiques), mais sur des données modifiées, en l’occurence, sur les rangs des données. La première conséquence évidente est une perte de puissance notable par rapport au test de Pearson. Cette perte de puissance peut être ici observée par le biais de la \\(p-\\)value plus élevée (donc moins significative) que pour le test précédent. Cela indique que même si la conclusion est la même, on rejette ici l’hypothèse nulle avec moins de confiance que pour le test de Pearson. Le \\(\\rho\\) de Spearman est équivalent au \\(r\\) de Pearson calculé sur les rangs des données. Lorsque plusieurs valeurs observées sont égales, plusieurs valeurs ont le même rang, ce qui cause l’apparition du message d’avertissement suivant : Impossible de calculer la p-value exacte avec des ex-aequos Ce message est sans conséquence tant que la \\(p-\\)value du test de Spearman est éloignée du seuil \\(\\alpha\\) (ce qui est le cas ici). Mais quand \\(p \\approx \\alpha\\), il faut être particulièrement prudent quant à l’interprétation qui est faite des résultats. Enfin, comme pour le test de Pearson, il est possible de réaliser un test de Spearman unilatéral en utilisant l’argument alternative = &quot;less&quot; ou alternative = &quot;greater&quot;. Les précautions à prendre pour utiliser ce genre de test sont toujours les mêmes. 5.2.5 Exercices 5.2.5.1 Canis lupus En 1970, le loup canis lupus a été éradiqué en Norvège et en Suède. Autour de 1980, un couple de loups, originaire d’une population plus à l’Est, a fondé une nouvelle population en Suède. En l’espace de 20 ans, cette population comptait approximativement 100 loups. Il y a toutefois fort à craindre qu’une population fondée par un si petit nombre d’individus souffre de consanguinité. Liberg et al. (2005) ont compilé les informations sur la reproduction dans cette population entre 1983 et 2002, et ils ont pu reconstruire le pédigrée des individus la composant. Ils ont ainsi été en mesure de déterminer avec précision le coefficient individuel de consanguinité dans 24 portées de louveteaux. Pour mémoire, le coefficient individuel de consanguinité vaut 0 si ses parents ne sont pas apparentés, 0.25 si ses parents sont frères et sœurs issus de grands-parents non apparentés, et plus de 0.25 si les associations consanguines se répètent depuis plusieurs générations. On souhaite déterminer si le coefficient de consanguinité est associé à la probabilité de survie des jeunes durant leur premier hiver. Les données de Liberg et al. (2005) sont disponibles dans le fichier loups.csv. La première colonne contient les coefficients de consanguinité et la seconde, le nombre de jeunes de chaque portée ayant survécu à leur premier hiver. Vous analyserez ces données en suivant l’ordre des étapes décrites plus haut. En particulier, vous prendrez soin de : Vérifier la qualité des données. Mettre les données dans un format approprié si besoin. Réaliser une exploration statistique puis visuelle des données. Vérifier les conditions d’application d’un test paramétrique. Faire le test approprié en posant les hypothèses nulles et alternatives judicieuses. Répondre à la question posée en intégrant tous les éléments utiles. 5.2.5.2 Les miracles de la mémoire À quel point les souvenirs d’évènements miraculeux sont-il fiables ? Une façon d’étudier cette question est de comparer différents récits de tours de magie extraordinaires. Parmis les tours les plus célèbres, on trouve celui de la corde du fakir. Dans l’une de ces versions, un magicien jette l’extrémité une corde d’apparence normale en l’air et cette corde devient rigide. Un garçon grimpe à la corde et finit par disparaître en haut de la scène. Le magicien lui demande de répondre mais n’obtient pas de réponse. Il attrape alors un couteau, grimpe à son tour, et le garçon, découpé en morceaux, tombe du ciel dans un panier posé par terre. Le magicien redescent de la corde et aide le garçon vivant, en un seul morceau et non blessé, à sortir du panier. Wiseman &amp; Lamont (1996) ont retrouvé 21 récits écrits de ce tour par des personnes ayant elles-mêmes assisté à ce tour. Ils ont attribué un score à chaque description selon le caractère plus ou moins impressionnant de la description. Par exemple, un score de 1 était attribué si le récit faisait état que “le garçon grimpe à la corde, puis il en redescend”. Les récits les plus impressionnants se sont vus attribuer la note de 5 (“le garçon grimpe, disparaît, est découpé en morceaux et réapparaît en chair et en os devant le public”). Pour chaque récit, les chercheurs ont également enregistré le nombre d’années écoulées entre le moment où le témoin a assisté au tour de magie, et le moment où il a consigné son récit par écrit. Y a-t’il un lien entre le caractère impressionnant (“impressiveness”) d’un souvenir et le temps écoulé jusqu’à l’écriture de sa description (“years”) ? Si oui, cela pourrait indiquer une tendance de la mémoire humaine à exagérer et à perdre en précision avec le temps. Les données de Wiseman &amp; Lamont (1996) sont disponibles dans le fichier ropetrick.csv. Importez ces données et analysez-les en respectant les consignes de l’exercice précédent. 5.3 Régression linéaire La régression est une méthode utilisée pour prédire les valeurs d’une variable numérique à partir des valeurs d’une seconde variable. Par exemple, le nuage de points de la figure ci-dessous montre comment la diversité génétique dans une population humaine locale peut être prédite par sa distance de dispersion depuis l’Est africain en ajustant une droite aux données (d’après Whitlock &amp; Schluter, 2015). L’homme moderne est apparu en Afrique et nos ancêtres ont perdu un peu de diversité génétique à chaque étape de leur colonisation de nouveaux territoires. Contrairement à la corrélation, ici, on n’examine pas seulement une éventuelle liaison : on suppose qu’une variable peut-être (en partie) expliquée par une autre. Nous aurons donc à distinguer les variables expliquées (ou dépendantes) qui figureront sur l’axe des y et seront nos variables prédites, et les variables explicatives (ou indépendantes) qui figureront sur l’axe des abscisses et seront les prédicteurs. Une façon de distinguer corrélation et régression consiste à dire que “corrélation n’est pas causalité”. Si on compare un nombre de variables suffisamment important, on finira toujours par en trouver qui seront corrélées. Cela ne veut pas dire pour autant qu’il existe un lien de cause à effet entre l’une et l’autre. Pour vous en convaincre, examinez cette page. Lorsque l’on s’intéresse à la régression linéaire, on essaie au contraire de prédire ou d’expliquer. En d’autres termes, on considère que les variations de la variable explicative sont au moins en partie la cause des variations de la variable expliquée. Lorsque l’on s’intéresse à la régression linéaire, on considère que la relation qui lie les deux variables est linéaire, et on souhaite quantifier l’intensité de la relation. Nous allons examiner maintenant comment faire ça dans R. 5.3.1 Exploration préalable des données Les activités humaines réduisent le nombre d’espèces dans un grand nombre d’écosystèmes à la surface du globe. Est-ce que cette diminution du nombre d’espèces affecte le fonctionnement de base des écosystèmes ? Où est-ce qu’au contraire, les espèces végétales sont majoritairement interchangeables, les fonctions des espèces disparues pouvants être assurées par les espèces toujours présentes ? Pour tenter de répondre à cette question, Tilman et al. (2006) ont ensemencé 161 parcelles de 9 mètres sur 9 mètres dans la réserve de Cedar Creek (Minesota, USA). Ils ont utilisé un nombre variable d’espèces typiques des prairies et ont mesuré la production de biomasse de chaque parcelle pendant 10 ans. Des lots de 1, 2, 4, 8 ou 16 plantes pluriannuelles (choisies au hasard parmi une liste de 18 espèces possibles) ont été assignés au hasard dans chacune des 161 parcelles. À l’issue des 10 années d’étude, les chercheurs ont mesuré un indice de stabilité de la biomasse en divisant la moyenne des biomasses sur 10 ans, par l’écart-type de ces mêmes biomasses. Les données de cette expérience sont disponibles dans le fichier plantbiomass.csv. 5.3.1.1 Importation, examen visuel et statistiques descriptives Comme toujours, on importe les données et on commence par un examen visuel afin de détecter les éventuels problèmes et pour savoir où l’on va. plant &lt;- read_csv(&quot;data/plantbiomass.csv&quot;) ## Parsed with column specification: ## cols( ## nSpecies = col_double(), ## biomassStability = col_double() ## ) plant ## # A tibble: 161 x 2 ## nSpecies biomassStability ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 7.47 ## 2 1 6.74 ## 3 1 6.61 ## 4 1 6.4 ## 5 1 5.67 ## 6 1 5.26 ## 7 1 4.8 ## 8 1 4.4 ## 9 1 4.4 ## 10 1 4.26 ## # … with 151 more rows Ce premier examen nous montre que nous disposons bien de 161 observations pour 2 variables : le nombre d’espèces présentes dans la parcelle pendant 10 ans, et l’indice de stabilité de la biomasse de chaque parcelle. skim(plant) ## ── Data Summary ──────────────────────── ## Values ## Name plant ## Number of rows 161 ## Number of columns 2 ## _______________________ ## Column type frequency: ## numeric 2 ## ________________________ ## Group variables None ## ## ── Variable type: numeric ──────────────────────────────────────────────────────────────────────────────────────────── ## skim_variable n_missing complete_rate mean sd p0 p25 p50 ## 1 nSpecies 0 1 6.32 5.64 1 2 4 ## 2 biomassStability 0 1 4.42 1.95 1.34 3.07 4 ## p75 p100 hist ## 1 8 16 ▇▁▂▁▃ ## 2 5.2 15.8 ▇▆▁▁▁ Ce premier examen nous montre que nous n’avons aucune données manquantes et que l’indice de stabilité a une distribution asymétrique (asymétrie droite) et qu’il varie d’un peu plus de 1 à près de 16. Pour en apprendre un peu plus, nous avons intérêt à examiner les données en groupes : plant %&gt;% group_by(nSpecies) %&gt;% skim() ## ── Data Summary ──────────────────────── ## Values ## Name Piped data ## Number of rows 161 ## Number of columns 2 ## _______________________ ## Column type frequency: ## numeric 1 ## ________________________ ## Group variables nSpecies ## ## ── Variable type: numeric ──────────────────────────────────────────────────────────────────────────────────────────── ## skim_variable nSpecies n_missing complete_rate mean sd p0 p25 ## 1 biomassStability 1 0 1 3.63 1.54 2.07 2.41 ## 2 biomassStability 2 0 1 3.83 1.35 1.34 2.97 ## 3 biomassStability 4 0 1 3.88 1.24 2.13 3 ## 4 biomassStability 8 0 1 4.62 1.19 2.53 3.75 ## 5 biomassStability 16 0 1 6.03 2.73 2.93 4.03 ## p50 p75 p100 hist ## 1 3.01 4.4 7.47 ▇▂▂▁▂ ## 2 3.73 4.51 7.41 ▃▇▆▂▁ ## 3 3.86 4.54 7.41 ▇▆▆▂▁ ## 4 4.40 5.03 7.74 ▃▇▃▃▁ ## 5 5.27 7.07 15.8 ▇▅▂▁▁ Cette fois, on obtient des informations pour chaque valeur de nombre d’espèce par parcelle. On constate par exemple que les moyennes de l’indice de stabilité de la biomasse augmentent très peu entre les catégories 1, 2 et 4 espèces par parcelle, mais que l’augmentation semble plus forte pour 8 et 16 espèces par parcelle. Tous les écarts-types semblent très proches, sauf peut-être pour la catégorie 16 espèces. Une visualisation des données est toujours indispensable : plant %&gt;% ggplot(aes(x = nSpecies, y = biomassStability)) + geom_point(alpha = 0.3) + labs(x = &quot;Nombre d&#39;espèces par parcelle&quot;, y = &quot;Indice de stabilité de la biomasse&quot;) + theme_bw() Ce graphique nous apprend plusieurs choses : Contrairement à la plupart des méthodes statistiques vues jusqu’ici, il n’est pas nécessaire que les données des variables soient distribuées selon une loi Normale. Ici, nous avons des données qui sont tout sauf normales pour la variable explicative puisque nous avons seulement les entiers 1, 2, 4, 8 et 16. Un histogramme ou une courbe de densité montre que la distribution de cette variable est très loin de la Normalité : plant %&gt;% ggplot(aes(x = nSpecies)) + geom_density(fill = &quot;steelblue&quot;, adjust = 0.2) + labs(x = &quot;Nombre d&#39;espèces par parcelle&quot;) + theme_bw() Cela n’est pas du tout problématique : comme pour l’ANOVA, les conditions d’application porteront sur les résidus de la régression, pas sur les variables elles-mêmes. Afin de limiter la sur-dispersion de la variable expliquée, notamment pour la catégorie 16 plantes par parcelle, nous allons transformer l’indice de stabilité en logarithme (attention, la fonction log() permet de calculer des logarithmes népériens ou logarithmes naturels) : plant &lt;- plant %&gt;% mutate(log_biomass = log(biomassStability)) plant ## # A tibble: 161 x 3 ## nSpecies biomassStability log_biomass ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 7.47 2.01 ## 2 1 6.74 1.91 ## 3 1 6.61 1.89 ## 4 1 6.4 1.86 ## 5 1 5.67 1.74 ## 6 1 5.26 1.66 ## 7 1 4.8 1.57 ## 8 1 4.4 1.48 ## 9 1 4.4 1.48 ## 10 1 4.26 1.45 ## # … with 151 more rows plant %&gt;% ggplot(aes(x = nSpecies, y = log_biomass)) + geom_point(alpha = 0.3) + labs(x = &quot;Nombre d&#39;espèces par parcelle&quot;, y = &quot;Transformation log\\n de l&#39;indice de stabilité de la biomasse&quot;) + theme_bw() On peut visualiser dès maintenant la droite de régression linéaire qui permet de lier ces deux variables grâce à la fonction geom_smooth(method = &quot;lm&quot;, se = FALSE)` : plant %&gt;% ggplot(aes(x = nSpecies, y = log_biomass)) + geom_point(alpha = 0.3) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(x = &quot;Nombre d&#39;espèces par parcelle&quot;, y = &quot;Transformation log\\n de l&#39;indice de stabilité de la biomasse&quot;) + theme_bw() À supposer que nous ayons le droit d’effectuer une régression linéaire (ce qu’il faudra vérifier avec les conditions d’application, après avoir fait la régression), la pente devrait être positive. 5.3.2 Le test paramétrique À une exception près, la procédure de régression linéaire est en tous points identique à l’analyse de variance. Quand on fait une ANOVA, la variable expliquée est numérique et la variable explicative est catégorielle (c’est un facteur). Quand on fait une régression linéaire, les 2 variables sont numériques. Pour le reste, tout est identique : on exprime la variable expliquée en fonction de la variable explicative et on vérifie après coup, grâce aux résidus, si nous avions le droit ou non de faire l’analyse. Pour faire une régression linéaire dans R, on utilise la fonction lm() (comme linear model). Et comme pour l’ANOVA, les résultats de l’analyse doivent être stockés dans un objet puisque cet objet contiendra tous les éléments utiles pour vérifier les conditions d’application : reg1 &lt;- lm(log_biomass ~ nSpecies, data = plant) Faire une régression linéaire avec cette commande revient à effectuer en même temps 2 tests d’hypothèses indépendants : le premier concerne l’ordonnée à l’origine de la droite de régression et le second concerne la pente de la droite de régression. Les hypothèses de ces tests sont les suivantes : Pour l’ordonnée à l’origine (“intercept” en anglais) : H\\(_0\\) : l’ordonnée à l’origine de la droite de régression vaut 0 dans la population générale. H\\(_1\\) : l’ordonnée à l’origine de la droite de régression est différente de 0 dans la population générale. Pour la pente (“slope” en anglais) : H\\(_0\\) : la pente de la droite de régression vaut 0 dans la population générale. H\\(_1\\) : la pente de la droite de régression est différente de 0 dans la population. générale 5.3.2.1 Résultats du test et interprétation Comme pour l’ANOVA, on affiche les résultats de ces tests à l’aide de la fonction summary() summary(reg1) ## ## Call: ## lm(formula = log_biomass ~ nSpecies, data = plant) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.97148 -0.25984 -0.00234 0.23100 1.03237 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.198294 0.041298 29.016 &lt; 2e-16 *** ## nSpecies 0.032926 0.004884 6.742 2.73e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3484 on 159 degrees of freedom ## Multiple R-squared: 0.2223, Adjusted R-squared: 0.2174 ## F-statistic: 45.45 on 1 and 159 DF, p-value: 2.733e-10 Dans la forme, ces résultats sont très proches de ceux de l’ANOVA. La rubrique Residuals donne des informations sommaires sur les résidus. Ces informations sont utiles puisque les résidus serviront à vérifier les conditions d’application de la régression. À ce stade, on regarde surtout si la médiane des résidus est proche de 0 et si les résidus sont à peu près symétriques (les premier et troisième quartiles ont à peu près la même valeur absolue, idem pour le minimum et le maximum). Le tableau Coefficients est celui qui nous intéresse le plus puisqu’il nous fournit, outre la réponse aux 2 tests, les estimations pour l’ordonnée à l’origine et la pente de la droite de régression. Ici, l’ordonnée à l’origine (intercept) est estimée à 1.198 (rappelez-vous que cette valeur fait référence au logarithme de la stabilité de la biomasse) et la pente à 0.033 (quand le nombre d’espèces augmente d’une unité, le logarithme de l’indice de stabilité de la biomasse augmente de 0.033 unités). Les \\(p-\\)values de chacun des 2 tests sont fournies dans la dernière colonne et sont ici très inférieures à \\(\\alpha\\) : on rejette donc les 2 hypothèses nulles. En particulier, puisque l’hypothèse nulle est rejetée pour le test qui concerne la pente de la droite, on peut considérer que le nombre de plantes dans les parcelles influence bel et bien l’indice de stabilité de la biomasse. Autrement dit, le nombre de plantes dans les parcelles, permet, dans une certaine mesure, de prédire la valeur de l’indice de stabilité de la biomasse. La relation n’est pas parfaite : le nombre de plantes dans chaque parcelle ne permet de prédire l’indice de stabilité de la biomasse que dans une mesure assez faible. C’est le Adjusted R-squared qui nous indique quelle est la “qualité” de prédiction du modèle. Ici, il vaut 0.22. Cela signifie que 22% des variations de l’indice de stabilité de la biomasse sont prédits par le nombre de plantes dans les parcelles. Une autre façon de présenter les choses consiste à dire que 78% des variations de l’indice de stabilité de biomasse sont expliqués par d’autres facteurs que le nombre d’espèces par parcelle. Le \\(R^2\\) (à en pas confondre avec le coefficient de corrélation \\(r\\)) renseigne donc sur la qualité de l’ajustement des données à la droite de régression. Il nous indique ici que le pouvoir prédictif de notre modèle linéaire est assez faible. Il est néanmoins significatif, ce qui indique que notre variable explicative joue bel et bien un rôle non négligeable dans les variations de la variable expliquée. 5.3.2.2 Intervalle de confiance de la régression La pente et l’ordonnée à l’origine de la droite de régression ont été obtenues à partir des données d’un échantillon (ici, 161 parcelles). Il s’agit donc d’estimations des pentes et ordonnées à l’origine de la relation plus générale qui concerne la population globale. Comme toute estimation, les pentes et ordonnées à l’origine de la droite de régression sont donc entâchées d’incertitudes. Nous pouvons quantifier ces incertitudes grâce au calcul des intervalles de confiance à 95% de ces 2 paramètres : confint(reg1) ## 2.5 % 97.5 % ## (Intercept) 1.11673087 1.27985782 ## nSpecies 0.02328063 0.04257117 Ces résultats nous indiquent que les valeurs d’ordonnées à l’origine les plus probables dans la population générale sont vraisemblablement comprises entre 1.117 et 1.280. De même, les valeurs de pentes les plus probables dans la population générale sont vraisemblablement situées dans l’intervalle [0.023 ; 0.043]. Il est possible de visualiser cette incertitude grâce à la fonction geom_smooth() utilisée plus tôt : plant %&gt;% ggplot(aes(x = nSpecies, y = log_biomass)) + geom_point(alpha = 0.3) + geom_smooth(method = &quot;lm&quot;, se = TRUE) + labs(x = &quot;Nombre d&#39;espèces par parcelle&quot;, y = &quot;Transformation log\\n de l&#39;indice de stabilité de la biomasse&quot;) + theme_bw() Cet intervalle d’incertitude correspond à l’incertitude de la moyenne de la variable expliquée pour une valeur donnée de la variable explicative. Ainsi, par exemple, si le nombre d’espèces d’une parcelle est égal à 8, alors, la régression et son incertitude associée nous dit que le logarithme de l’indice de stabilité de la biomasse vaudra en moyenne environ 1.46, avec un intervalle de confiance de [1.40 ; 1.51] 5.3.2.3 Conditions d’application Les conditions d’application de la régression sont les mêmes que celles de l’ANOVA. Je vous renvoie donc au chapitre 4.2.2.2 pour savoir quelles sont ces conditions d’application et comment les vérifier. J’insiste bien sur le fait que les conditions d’application sont absolument identiques à celles de l’ANOVA. Si je fais ici l’économie de la description, vous ne devez jamais faire l’économie de la vérification des conditions d’application. par(mfrow = c(2, 2)) plot(reg1) par(mfrow = c(1, 1)) C’est seulement après avoir réalisé, examiné et commenté ces graphiques que vous serez en mesure de dire si oui ou non vous aviez le droit de faire la régression linéaire, et donc d’en interpréter les résultats. 5.3.3 L’alternative non paramétrique Lorsque les conditions d’application de la régression linéaire ne sont pas vérifiées, on a principalement deux options : On essaie de transformer les données afin que les résidus de la régression se comportent mieux. Cela signifie tester différents types de transformations, ce qui peut être chronophage pour un résultat pas toujours garanti. On utilise d’autre types de modèles de régression, en particulier les modèles de régressions linéaires généralisées (GLM), qui s’accommodent très bien de résidus non normaux et/ou non homogènes. Mais il s’agit là d’une toute autre classe de méthodes qui ne sont pas au programme de la licence. 5.3.4 Exercices 5.3.4.1 Le quartet d’Anscombe Exécutez la commande suivante (il vous faudra peut-être presser la touche Entrée dans la console) pour produire les 4 graphiques d’Anscombe : example(anscombe) Examinez attentivement les nombreux résultats produits par cette commande dans la console, ainsi que les 4 graphiques obtenus. Vous devriez remarquer que pour ces 4 jeux de données, 2 variables numériques sont mises en relation. Les variables x1, x2, x3 et x4 ont toutes la même moyenne et la même variance. Les variables y1, y2, y3 et y4 ont toutes la même moyenne et la même variance. Les coefficients de corrélation entre x1 et y1, x2 et y2, x3 et y3, x4 et y4 sont tous les mêmes. Enfin, les ordonnées à l’origine et les pentes de ces 4 droites de régression linéaires sont identiques. Pourtant, seule la première régression linéaire est valide. Pourquoi ? 5.3.4.2 In your face Les hommes ont en moyenne un ratio “largeur du visage sur longueur du visage” supérieur à celui des femmes. Cela reflète des niveaux d’expression de la testostérone différents entre hommes et femmes au moment de la puberté. On sait aussi que les niveaux de testosterone permettent de prédire, dans une certaine mesure, l’agressivité chez les mâles de nombreuses espèces. On peut donc poser la question suivante : la forme du visage permet-elle de prédire l’agressivité ? Pour tester cela, Carré &amp; McCormick (2008) ont suivi 21 joueurs de hockey sur glace au niveau universitaire. Ils ont tout d’abord mesuré le ratio largeur du visage sur longueur du visage de chaque sujet, puis, ils ont compté le nombre moyen de minutes de pénalité par match reçu par chaque sujet au cours de la saison, en se limitant aux pénalités infligées pour cause de brutalité. Les données sont fournies dans le fichier hockey.csv. Importez, examinez et analysez ces données pour répondre à la question posée. References "]
]
