<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5 Séance 4 : corrélations et régressions | Travaux Pratiques de Biométrie 3</title>
  <meta name="description" content="Ce livre est un document permettant de réaliser des analyses statistiques descriptives, des tests d’hypothèse, des régressions linéaires et des analyses de variances dans le cadre des travaux pratiques de Biométrie 3." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="5 Séance 4 : corrélations et régressions | Travaux Pratiques de Biométrie 3" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://besibo.github.io/Biometrie3/" />
  
  <meta property="og:description" content="Ce livre est un document permettant de réaliser des analyses statistiques descriptives, des tests d’hypothèse, des régressions linéaires et des analyses de variances dans le cadre des travaux pratiques de Biométrie 3." />
  <meta name="github-repo" content="besibo/Biometrie3" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5 Séance 4 : corrélations et régressions | Travaux Pratiques de Biométrie 3" />
  
  <meta name="twitter:description" content="Ce livre est un document permettant de réaliser des analyses statistiques descriptives, des tests d’hypothèse, des régressions linéaires et des analyses de variances dans le cadre des travaux pratiques de Biométrie 3." />
  

<meta name="author" content="Benoît Simon-Bouhet" />


<meta name="date" content="2022-01-26" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="séance-3-comparer-la-moyenne-de-plus-de-2-groupes.html"/>
<link rel="next" href="references.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    background-color: #ffffff;
    color: #a0a0a0;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #a0a0a0;  padding-left: 4px; }
div.sourceCode
  { color: #1f1c1b; background-color: #ffffff; }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span. { color: #1f1c1b; } /* Normal */
code span.al { color: #bf0303; background-color: #f7e6e6; font-weight: bold; } /* Alert */
code span.an { color: #ca60ca; } /* Annotation */
code span.at { color: #0057ae; } /* Attribute */
code span.bn { color: #b08000; } /* BaseN */
code span.bu { color: #644a9b; font-weight: bold; } /* BuiltIn */
code span.cf { color: #1f1c1b; font-weight: bold; } /* ControlFlow */
code span.ch { color: #924c9d; } /* Char */
code span.cn { color: #aa5500; } /* Constant */
code span.co { color: #898887; } /* Comment */
code span.cv { color: #0095ff; } /* CommentVar */
code span.do { color: #607880; } /* Documentation */
code span.dt { color: #0057ae; } /* DataType */
code span.dv { color: #b08000; } /* DecVal */
code span.er { color: #bf0303; text-decoration: underline; } /* Error */
code span.ex { color: #0095ff; font-weight: bold; } /* Extension */
code span.fl { color: #b08000; } /* Float */
code span.fu { color: #644a9b; } /* Function */
code span.im { color: #ff5500; } /* Import */
code span.in { color: #b08000; } /* Information */
code span.kw { color: #1f1c1b; font-weight: bold; } /* Keyword */
code span.op { color: #1f1c1b; } /* Operator */
code span.ot { color: #006e28; } /* Other */
code span.pp { color: #006e28; } /* Preprocessor */
code span.re { color: #0057ae; background-color: #e0e9f8; } /* RegionMarker */
code span.sc { color: #3daee9; } /* SpecialChar */
code span.ss { color: #ff5500; } /* SpecialString */
code span.st { color: #bf0303; } /* String */
code span.va { color: #0057ae; } /* Variable */
code span.vs { color: #bf0303; } /* VerbatimString */
code span.wa { color: #bf0303; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Biométrie 3</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Préambule</a></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a></li>
<li class="chapter" data-level="3" data-path="seance1.html"><a href="seance1.html"><i class="fa fa-check"></i><b>3</b> Séances 1 et 2 : statistiques descriptives et tests d’hypothèses</a><ul>
<li class="chapter" data-level="3.1" data-path="seance1.html"><a href="seance1.html#packages"><i class="fa fa-check"></i><b>3.1</b> Packages et données</a></li>
<li class="chapter" data-level="3.2" data-path="seance1.html"><a href="seance1.html#comparaison-de-la-moyenne-dune-population-à-une-valeur-théorique"><i class="fa fa-check"></i><b>3.2</b> Comparaison de la moyenne d’une population à une valeur théorique</a><ul>
<li class="chapter" data-level="3.2.1" data-path="seance1.html"><a href="seance1.html#Explo"><i class="fa fa-check"></i><b>3.2.1</b> Exploration préalable des données</a></li>
<li class="chapter" data-level="3.2.2" data-path="seance1.html"><a href="seance1.html#le-test-paramétrique"><i class="fa fa-check"></i><b>3.2.2</b> Le test paramétrique</a></li>
<li class="chapter" data-level="3.2.3" data-path="seance1.html"><a href="seance1.html#lalternative-non-paramétrique"><i class="fa fa-check"></i><b>3.2.3</b> L’alternative non paramétrique</a></li>
<li class="chapter" data-level="3.2.4" data-path="seance1.html"><a href="seance1.html#exercice-dapplication"><i class="fa fa-check"></i><b>3.2.4</b> Exercice d’application</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="seance1.html"><a href="seance1.html#comparaison-de-la-moyenne-de-2-populations-données-appariées"><i class="fa fa-check"></i><b>3.3</b> Comparaison de la moyenne de 2 populations : données appariées</a><ul>
<li class="chapter" data-level="3.3.1" data-path="seance1.html"><a href="seance1.html#Explo2"><i class="fa fa-check"></i><b>3.3.1</b> Exploration préalable des données</a></li>
<li class="chapter" data-level="3.3.2" data-path="seance1.html"><a href="seance1.html#le-test-paramétrique-1"><i class="fa fa-check"></i><b>3.3.2</b> Le test paramétrique</a></li>
<li class="chapter" data-level="3.3.3" data-path="seance1.html"><a href="seance1.html#lalternative-non-paramétrique-1"><i class="fa fa-check"></i><b>3.3.3</b> L’alternative non paramétrique</a></li>
<li class="chapter" data-level="3.3.4" data-path="seance1.html"><a href="seance1.html#exercice-dapplication-1"><i class="fa fa-check"></i><b>3.3.4</b> Exercice d’application</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="seance1.html"><a href="seance1.html#Indep"><i class="fa fa-check"></i><b>3.4</b> Comparaison de la moyenne de 2 populations : échantillons indépendants</a><ul>
<li class="chapter" data-level="3.4.1" data-path="seance1.html"><a href="seance1.html#exploration-préalable-des-données"><i class="fa fa-check"></i><b>3.4.1</b> Exploration préalable des données</a></li>
<li class="chapter" data-level="3.4.2" data-path="seance1.html"><a href="seance1.html#le-test-paramétrique-2"><i class="fa fa-check"></i><b>3.4.2</b> Le test paramétrique</a></li>
<li class="chapter" data-level="3.4.3" data-path="seance1.html"><a href="seance1.html#lalternative-non-paramétrique-2"><i class="fa fa-check"></i><b>3.4.3</b> L’alternative non paramétrique</a></li>
<li class="chapter" data-level="3.4.4" data-path="seance1.html"><a href="seance1.html#exercice-dapplication-2"><i class="fa fa-check"></i><b>3.4.4</b> Exercice d’application</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="seance1.html"><a href="seance1.html#tests-bilatéraux-et-unilatéraux"><i class="fa fa-check"></i><b>3.5</b> Tests bilatéraux et unilatéraux</a><ul>
<li class="chapter" data-level="3.5.1" data-path="seance1.html"><a href="seance1.html#principe"><i class="fa fa-check"></i><b>3.5.1</b> Principe</a></li>
<li class="chapter" data-level="3.5.2" data-path="seance1.html"><a href="seance1.html#un-exemple-pas-à-pas"><i class="fa fa-check"></i><b>3.5.2</b> Un exemple pas à pas</a></li>
<li class="chapter" data-level="3.5.3" data-path="seance1.html"><a href="seance1.html#exercice-dapplication-3"><i class="fa fa-check"></i><b>3.5.3</b> Exercice d’application</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="séance-3-comparer-la-moyenne-de-plus-de-2-groupes.html"><a href="séance-3-comparer-la-moyenne-de-plus-de-2-groupes.html"><i class="fa fa-check"></i><b>4</b> Séance 3 : comparer la moyenne de plus de 2 groupes</a><ul>
<li class="chapter" data-level="4.1" data-path="séance-3-comparer-la-moyenne-de-plus-de-2-groupes.html"><a href="séance-3-comparer-la-moyenne-de-plus-de-2-groupes.html#packages-et-données"><i class="fa fa-check"></i><b>4.1</b> Packages et données</a></li>
<li class="chapter" data-level="4.2" data-path="séance-3-comparer-la-moyenne-de-plus-de-2-groupes.html"><a href="séance-3-comparer-la-moyenne-de-plus-de-2-groupes.html#lanalyse-de-variance-à-un-facteur"><i class="fa fa-check"></i><b>4.2</b> L’analyse de variance à un facteur</a><ul>
<li class="chapter" data-level="4.2.1" data-path="séance-3-comparer-la-moyenne-de-plus-de-2-groupes.html"><a href="séance-3-comparer-la-moyenne-de-plus-de-2-groupes.html#exploration-préalable-des-données-1"><i class="fa fa-check"></i><b>4.2.1</b> Exploration préalable des données</a></li>
<li class="chapter" data-level="4.2.2" data-path="séance-3-comparer-la-moyenne-de-plus-de-2-groupes.html"><a href="séance-3-comparer-la-moyenne-de-plus-de-2-groupes.html#le-test-paramétrique-3"><i class="fa fa-check"></i><b>4.2.2</b> Le test paramétrique</a></li>
<li class="chapter" data-level="4.2.3" data-path="séance-3-comparer-la-moyenne-de-plus-de-2-groupes.html"><a href="séance-3-comparer-la-moyenne-de-plus-de-2-groupes.html#lalternative-non-paramétrique-3"><i class="fa fa-check"></i><b>4.2.3</b> L’alternative non paramétrique</a></li>
<li class="chapter" data-level="4.2.4" data-path="séance-3-comparer-la-moyenne-de-plus-de-2-groupes.html"><a href="séance-3-comparer-la-moyenne-de-plus-de-2-groupes.html#exercices-dapplication"><i class="fa fa-check"></i><b>4.2.4</b> Exercices d’application</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="séance-4-corrélations-et-régressions.html"><a href="séance-4-corrélations-et-régressions.html"><i class="fa fa-check"></i><b>5</b> Séance 4 : corrélations et régressions</a><ul>
<li class="chapter" data-level="5.1" data-path="séance-4-corrélations-et-régressions.html"><a href="séance-4-corrélations-et-régressions.html#packages-et-données-1"><i class="fa fa-check"></i><b>5.1</b> Packages et données</a></li>
<li class="chapter" data-level="5.2" data-path="séance-4-corrélations-et-régressions.html"><a href="séance-4-corrélations-et-régressions.html#corrélation"><i class="fa fa-check"></i><b>5.2</b> Corrélation</a><ul>
<li class="chapter" data-level="5.2.1" data-path="séance-4-corrélations-et-régressions.html"><a href="séance-4-corrélations-et-régressions.html#principe-1"><i class="fa fa-check"></i><b>5.2.1</b> Principe</a></li>
<li class="chapter" data-level="5.2.2" data-path="séance-4-corrélations-et-régressions.html"><a href="séance-4-corrélations-et-régressions.html#exploration-préalable-des-données-2"><i class="fa fa-check"></i><b>5.2.2</b> Exploration préalable des données</a></li>
<li class="chapter" data-level="5.2.3" data-path="séance-4-corrélations-et-régressions.html"><a href="séance-4-corrélations-et-régressions.html#le-test-paramétrique-4"><i class="fa fa-check"></i><b>5.2.3</b> Le test paramétrique</a></li>
<li class="chapter" data-level="5.2.4" data-path="séance-4-corrélations-et-régressions.html"><a href="séance-4-corrélations-et-régressions.html#lalternative-non-paramétrique-4"><i class="fa fa-check"></i><b>5.2.4</b> L’alternative non paramétrique</a></li>
<li class="chapter" data-level="5.2.5" data-path="séance-4-corrélations-et-régressions.html"><a href="séance-4-corrélations-et-régressions.html#exercices"><i class="fa fa-check"></i><b>5.2.5</b> Exercices</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="séance-4-corrélations-et-régressions.html"><a href="séance-4-corrélations-et-régressions.html#régression-linéaire"><i class="fa fa-check"></i><b>5.3</b> Régression linéaire</a><ul>
<li class="chapter" data-level="5.3.1" data-path="séance-4-corrélations-et-régressions.html"><a href="séance-4-corrélations-et-régressions.html#exploration-préalable-des-données-3"><i class="fa fa-check"></i><b>5.3.1</b> Exploration préalable des données</a></li>
<li class="chapter" data-level="5.3.2" data-path="séance-4-corrélations-et-régressions.html"><a href="séance-4-corrélations-et-régressions.html#le-test-paramétrique-5"><i class="fa fa-check"></i><b>5.3.2</b> Le test paramétrique</a></li>
<li class="chapter" data-level="5.3.3" data-path="séance-4-corrélations-et-régressions.html"><a href="séance-4-corrélations-et-régressions.html#lalternative-non-paramétrique-5"><i class="fa fa-check"></i><b>5.3.3</b> L’alternative non paramétrique</a></li>
<li class="chapter" data-level="5.3.4" data-path="séance-4-corrélations-et-régressions.html"><a href="séance-4-corrélations-et-régressions.html#exercices-1"><i class="fa fa-check"></i><b>5.3.4</b> Exercices</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Publié avec bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Travaux Pratiques de Biométrie 3</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="séance-4-corrélations-et-régressions" class="section level1">
<h1><span class="header-section-number">5</span> Séance 4 : corrélations et régressions</h1>
<div id="packages-et-données-1" class="section level2">
<h2><span class="header-section-number">5.1</span> Packages et données</h2>
<p>Comme dans tous les chapitres de cet ouvrage, nous aurons besoin ici du <code>tidyverse</code> et du package <code>skimr</code>. Noubliez pas de les charger en mémoire avant d’aller plus loin.</p>
<div class="sourceCode" id="cb159"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb159-1"><a href="séance-4-corrélations-et-régressions.html#cb159-1"></a><span class="kw">library</span>(tidyverse)</span>
<span id="cb159-2"><a href="séance-4-corrélations-et-régressions.html#cb159-2"></a><span class="kw">library</span>(skimr)</span></code></pre></div>
<p>Vous aurez également besoin des jeux de données suivants, disponibles sur l’ENT ou directement téléchargeables depuis ce document :</p>
<ul>
<li><a href="https://besibo.github.io/Biometrie3/data/birds.csv"><code>birds.csv</code></a></li>
<li><a href="https://besibo.github.io/Biometrie3/data/loups.csv"><code>loups.csv</code></a></li>
<li><a href="https://besibo.github.io/Biometrie3/data/ropetrick.csv"><code>ropetrick.csv</code></a></li>
<li><a href="https://besibo.github.io/Biometrie3/data/plantbiomass.csv"><code>plantbiomass.csv</code></a></li>
<li><a href="https://besibo.github.io/Biometrie3/data/hockey.csv"><code>hockey.csv</code></a></li>
</ul>
</div>
<div id="corrélation" class="section level2">
<h2><span class="header-section-number">5.2</span> Corrélation</h2>
<div id="principe-1" class="section level3">
<h3><span class="header-section-number">5.2.1</span> Principe</h3>
<p>Lorsque des variables numériques sont associées ont dit qu’elles sont <strong>corrélées</strong>. Par exemple, la taille du cerveau et la taille du corps sont corrélées positivement parmi les espèces de mammifères. Les espèces de grande taille ont tendance à avoir un cerveau plus grand et les petites espèces ont tendance à avoir un cerveau plus petit.<br />
Le coefficient de corrélation est la quantité qui décrit la force et la direction de l’association entre deux variables numériques mesurées sur un échantillon de sujets ou d’unités d’observation. La corrélation reflète la quantité de dispersion dans un nuage de points entre deux variables. Contrairement à la régression linéaire, la corrélation n’ajuste aucune droite à des données et ne permet donc pas de mesurer à quel point le changement d’une variable entraîne un changement rapide ou lent de l’autre variable.</p>
<p>Ainsi, sur la figure ci-dessous, le coefficient de corrélation entre <code>X</code> et <code>Y</code> est le même pour les deux graphiques : il vaut 1.</p>
<p><img src="figure/unnamed-chunk-96-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>Ici, le coefficient de corrélation (noté <span class="math inline">\(r\)</span>) vaut 1 dans les deux cas, car tous les points sont alignés sur une droite. La pente de la droite n’influence en rien la valeur de corrélation. En revanche, la dispersion des points autour d’une droite parfaite a une influence :</p>
<p><img src="figure/unnamed-chunk-97-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>Plus la dispersion autour d’une droite parfaite sera grande, plus la corrélation sera faible. C’est la raison pour laquelle lorsque l’on parle de “corrélation”, on sous-entend généralement <strong>corrélation linéaire</strong>. Ainsi, 2 variables peuvent avoir une relation très forte, mais un coefficient de corrélation nul, si leur relation n’est pas linéaire :</p>
<p><img src="figure/unnamed-chunk-98-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>L’exploration graphique de vos données devrait donc toujours être une priorité. Calculer un coefficient de corrélation nul ou très faible ne signifie par pour autant une absence de relation entre les 2 variables numériques étudiées. Cela peut signifier une relation non linéaire. La solution la plus simple pour distinguer une relation telle que celle du graphique précédent, et une absence de relation telle que celle présentée dans le graphique ci-dessous, est l’examen visuel des données :</p>
<p><img src="figure/unnamed-chunk-99-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>En bref, le coefficient de corrélation <span class="math inline">\(r\)</span> est compris entre -1 et +1 :</p>
<ul>
<li>Une forte valeur absolue (<span class="math inline">\(r\)</span> proche de -1 ou +1), indique une relation presque linéaire.</li>
<li>Une faible valeur absolue indique soit une absence de relation, soit une relation non linéaire (la visualisation graphique permet généralement d’en savoir plus).</li>
<li>Une valeur positive indique qu’une augmentation de la première variable est associée à une augmentation de la seconde variable.</li>
<li>Une valeur négative indique qu’une augmentation de la première variable est associée à une diminution de la seconde variable.</li>
</ul>
<p>Dans la suite de ce chapitre, nous allons voir comment calculer le coefficient de corrélation entre 2 variables numériques, et puisque nous travaillons avec des <strong>échantillons</strong>, ce calcul sera nécessairement entaché d’<strong>incertitude</strong>. Tout comme la moyenne ou la variance d’un échantillon, la corrélation est un paramètre des populations dont nous ne pourrons qu’estimer la valeur. Toute estimation de corrélation devra donc être encadrée par un intervalle d’incertitude, généralement, il s’agit de l’intervalle de confiance à 95% de la corrélation. Enfin, outre l’estimation de la valeur de la corrélation et de son incertitude, nous pourrons aussi faire des tests d’hypothèses au sujet des corrélations que nous estimerons.</p>
</div>
<div id="exploration-préalable-des-données-2" class="section level3">
<h3><span class="header-section-number">5.2.2</span> Exploration préalable des données</h3>
<div id="importation-et-examen-visuel-4" class="section level4">
<h4><span class="header-section-number">5.2.2.1</span> Importation et examen visuel</h4>
<p>Les adultes qui infligent des mauvais traitements à leurs enfants ont souvent été maltraités dans leur enfance. Une telle relation exsite-t-elle également chez d’autres espèces animales, chez qui cette relation pourrait être étudiée plus facilement ? <span class="citation">Müller et al. (<a href="#ref-muller2011" role="doc-biblioref">2011</a>)</span> ont étudié cette possibilité chez le fou de Grant (<em>Sula granti</em>), un oiseau marin colonial vivant entre autres aux Galápagos. Les jeunes laissés au nid sans attention parentale reçoivent fréquemment la visite d’autres oiseaux, qui se comportent souvent de manière agressive à leur encontre. Les chercheurs ont compté le nombre de ces visites dans le nid de 24 poussins dotés d’une bague d’identification individuelle. Ces 24 individus ont ensuite été suivis à l’âge adulte, lorsqu’ils sont à leur tour devenus parents. Les données récoltées par les chercheurs figurent dans le fichier <a href="https://besibo.github.io/Biometrie3/data/birds.csv"><code>birds.csv</code></a>. Importez ces données dans <code>R</code> dans un objet noté <code>birds</code>.</p>
<div class="sourceCode" id="cb160"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb160-1"><a href="séance-4-corrélations-et-régressions.html#cb160-1"></a>birds</span></code></pre></div>
<pre><code># A tibble: 24 × 2
   nVisitsNestling futureBehavior
             &lt;dbl&gt;          &lt;dbl&gt;
 1               1          -0.8 
 2               7          -0.92
 3              15          -0.8 
 4               4          -0.46
 5              11          -0.47
 6              14          -0.46
 7              23          -0.23
 8              14          -0.16
 9               9          -0.23
10               5          -0.23
# … with 14 more rows</code></pre>
<p>La première colonne de ce tableau indique, pour chaque individu suivi, le nombre de visites reçues au nid de la part d’adultes agressifs lorsqu’ils étaient poussins. La seconde colonne indique, pour ces mêmes individus devenus adultes, le nombre de visites agressives effectuées à des nids d’autres poussins. Ce nombre n’est pas dans la même unité que la première variable car il a été corrigé par d’autres variables d’intérêt pour les chercheurs.</p>
<p>Il manque à ce tableau une variable indiquant le code des individus. Elle n’est pas indispensable, mais la rajouter est une bonne habitude à prendre pour toujours travailler avec des “données rangées” :</p>
<div class="sourceCode" id="cb162"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb162-1"><a href="séance-4-corrélations-et-régressions.html#cb162-1"></a>birds &lt;-<span class="st"> </span>birds <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb162-2"><a href="séance-4-corrélations-et-régressions.html#cb162-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">ID =</span> <span class="kw">factor</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">24</span>))</span>
<span id="cb162-3"><a href="séance-4-corrélations-et-régressions.html#cb162-3"></a>birds</span></code></pre></div>
<pre><code># A tibble: 24 × 3
   nVisitsNestling futureBehavior ID   
             &lt;dbl&gt;          &lt;dbl&gt; &lt;fct&gt;
 1               1          -0.8  1    
 2               7          -0.92 2    
 3              15          -0.8  3    
 4               4          -0.46 4    
 5              11          -0.47 5    
 6              14          -0.46 6    
 7              23          -0.23 7    
 8              14          -0.16 8    
 9               9          -0.23 9    
10               5          -0.23 10   
# … with 14 more rows</code></pre>
</div>
<div id="statistiques-descriptives-4" class="section level4">
<h4><span class="header-section-number">5.2.2.2</span> Statistiques descriptives</h4>
<p>Comme toujours, la première chose à faire est d’examiner quelques statistiques descriptives pour se faire une idée de la forme des données et pour repérer les éventuelles données manquantes ou aberrantes.</p>
<div class="sourceCode" id="cb164"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb164-1"><a href="séance-4-corrélations-et-régressions.html#cb164-1"></a><span class="kw">skim</span>(birds)</span></code></pre></div>
<pre><code>── Data Summary ────────────────────────
                           Values
Name                       birds 
Number of rows             24    
Number of columns          3     
_______________________          
Column type frequency:           
  factor                   1     
  numeric                  2     
________________________         
Group variables            None  

── Variable type: factor ───────────────────────────────────────────────────────
  skim_variable n_missing complete_rate ordered n_unique top_counts            
1 ID                    0             1 FALSE         24 1: 1, 2: 1, 3: 1, 4: 1

── Variable type: numeric ──────────────────────────────────────────────────────
  skim_variable   n_missing complete_rate   mean    sd    p0    p25   p50    p75
1 nVisitsNestling         0             1 13.1   7.21   1     8.75   13   15.8  
2 futureBehavior          0             1 -0.119 0.374 -0.92 -0.288  -0.1  0.182
   p100 hist 
1 31    ▅▇▅▃▁
2  0.39 ▂▂▃▂▇</code></pre>
<p>Outre le facteur <code>ID</code> que nous venons de créer, nous disposons donc de 2 variables numériques qui ne contiennent pas de données manquantes.</p>
<ol style="list-style-type: decimal">
<li>La variable <code>nVisitsNestling</code>, qui indique le nombre de visites agressives reçues par les individus suivis lorsqu’ils étaient de jeunes poussins, varie de 1 à 31, pour une moyenne de 13.12, une médiane proche (13) mais un écart-type important.</li>
<li>La variable <code>futureBehavior</code> varie de -0.92 à 0.39, avec une moyenne et une médiane proche de 0 (-0.12 et -0.1 respectivement).</li>
</ol>
<p>À ce stade, il est possible de calculer le coefficient de corrélation linéaire entre les 2 variables :</p>
<div class="sourceCode" id="cb166"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb166-1"><a href="séance-4-corrélations-et-régressions.html#cb166-1"></a>birds <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb166-2"><a href="séance-4-corrélations-et-régressions.html#cb166-2"></a><span class="st">  </span><span class="kw">select</span>(nVisitsNestling, futureBehavior) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb166-3"><a href="séance-4-corrélations-et-régressions.html#cb166-3"></a><span class="st">  </span><span class="kw">cor</span>()</span></code></pre></div>
<pre><code>                nVisitsNestling futureBehavior
nVisitsNestling       1.0000000      0.5337225
futureBehavior        0.5337225      1.0000000</code></pre>
<p>Le résultat est obtenu sous la forme d’une matrice symétrique :</p>
<ul>
<li>Sur la diagonale, les corrélations valent 1 (le coefficient de corrélation d’une variable avec elle-même vaut toujours 1).</li>
<li>En dehors de la diagonale, on trouve le coefficient de corrélation linéaire entre les 2 variables d’intérêt. Ici, il est positif et vaut 0.534, ce qui est une valeur relativement élevée dans le domaine de la biologie ou de l’écologie.</li>
</ul>
</div>
<div id="exploration-graphique-4" class="section level4">
<h4><span class="header-section-number">5.2.2.3</span> Exploration graphique</h4>
<p>Afin de savoir si la valeur moyenne de <span class="math inline">\(r\)</span> calculée précédemment reflète une relation linéaire mais moyenne, ou une relation qui n’est pas vraiment linéaire, nous devons faire un nuage de points :</p>
<div class="sourceCode" id="cb168"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb168-1"><a href="séance-4-corrélations-et-régressions.html#cb168-1"></a>birds <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb168-2"><a href="séance-4-corrélations-et-régressions.html#cb168-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> nVisitsNestling, <span class="dt">y =</span> futureBehavior)) <span class="op">+</span></span>
<span id="cb168-3"><a href="séance-4-corrélations-et-régressions.html#cb168-3"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb168-4"><a href="séance-4-corrélations-et-régressions.html#cb168-4"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Nombre de visites reçues par le poussin&quot;</span>,</span>
<span id="cb168-5"><a href="séance-4-corrélations-et-régressions.html#cb168-5"></a>       <span class="dt">y =</span> <span class="st">&quot;Comportement futur&quot;</span>) <span class="op">+</span></span>
<span id="cb168-6"><a href="séance-4-corrélations-et-régressions.html#cb168-6"></a><span class="st">  </span><span class="kw">theme_bw</span>()</span></code></pre></div>
<p><img src="figure/unnamed-chunk-104-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>On constate ici que la corrélation moyenne obtenue plus haut est due au fait que les points sont assez dispersés, et non au fait que la relation n’est pas linéaire. On peut donc dire que la relation, si elle existe, n’est pas parfaite. Le comportement des individus devenus adultes semble donc en partie lié au nombre de visites agessives qu’ils ont reçues étant jeunes, mais ce n’est certainement pas le seul facteur influençant leur comportement. Un test d’hypothèses devrait nous permettre de déterminer si la corrélation linéaire observée ici est significativement différente de 0 ou non.</p>
</div>
</div>
<div id="le-test-paramétrique-4" class="section level3">
<h3><span class="header-section-number">5.2.3</span> Le test paramétrique</h3>
<p>Comme pour la plupart des grandeurs calculées à partir d’un échantillon, la corrélation <span class="math inline">\(r\)</span> n’est qu’un estimateur de la corrélation qui existe réellement entre ces deux variables dans la population générale. Dans la population générale, la corrélation linéaire est généralement notée <span class="math inline">\(\rho\)</span>. Son estimateur, <span class="math inline">\(r\)</span> est donc souvent noté <span class="math inline">\(\hat{\rho}\)</span>.</p>
<p>Le test d’hypothèses que nous allons faire maintenant permet de vérifier si le coefficient de corrélation <span class="math inline">\(\rho\)</span> dans la population générale est différent de 0 ou non. Les hypothèses de ce test sont les suivantes :</p>
<ul>
<li>H<span class="math inline">\(_0\)</span> : le coefficient de corrélation entre les deux variables étudiées vaut 0 dans la population générale (<span class="math inline">\(\rho = 0\)</span>).</li>
<li>H<span class="math inline">\(_1\)</span> : le coefficient de corrélation entre les deux variables étudiées est différent de 0 dans la population générale (<span class="math inline">\(\rho \neq 0\)</span>).</li>
</ul>
<p>Ce test est réalisé dans <code>R</code> grâce à la fonction <code>cor.test()</code>.</p>
<div id="résultats-du-test-et-interprétation" class="section level4">
<h4><span class="header-section-number">5.2.3.1</span> Résultats du test et interprétation</h4>
<div class="sourceCode" id="cb169"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb169-1"><a href="séance-4-corrélations-et-régressions.html#cb169-1"></a><span class="kw">cor.test</span>(birds<span class="op">$</span>nVisitsNestling, birds<span class="op">$</span>futureBehavior)</span></code></pre></div>
<pre><code>
    Pearson&#39;s product-moment correlation

data:  birds$nVisitsNestling and birds$futureBehavior
t = 2.9603, df = 22, p-value = 0.007229
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 0.1660840 0.7710999
sample estimates:
      cor 
0.5337225 </code></pre>
<p>Le test réalisé ici est le <strong>test de corrélation de Pearson</strong>. Il s’agit d’un test paramétrique dont les conditions d’application sont expliquées plus bas. Comme pour tous les tests examinés jusqu’ici, les premières lignes des résultats fournissent toutes les informations utiles au sujet du test. Ici, on peut dire :</p>
<blockquote>
<p>Au seuil <span class="math inline">\(\alpha = 0.05\)</span>, le test de corrélation de Pearson a permis de rejeter l’hypothèse nulle selon laquelle le nombre de visites agressives au nid des poussins et leur futur comportement agressif sont indépendants (<span class="math inline">\(t = 2.96\)</span>, <span class="math inline">\(ddl = 22\)</span>, <span class="math inline">\(p = 0.007\)</span>).</p>
</blockquote>
<p>Ce test prouve donc que <span class="math inline">\(\rho\)</span> est significativement différent de 0. La valeur de 0.53 observée ici n’est pas due au hasard de l’échantillonnage.</p>
<p>Comme toujours, les résultats du test que nous avons réalisé ne nous disent rien de la valeur de la corrélation estimée, ni de son incertitude. Il nous faut pour cela examiner les autres lignes fournies par <code>R</code> lorsque nous faisons ce test et qui relèvent de l’estimation (voir section suivante).</p>
<p>Dernière chose concernant ce test, nous avons fait ici un test bilatéral comme nous le rappelle cette ligne des résultats :</p>
<p><code>alternative hypothesis: true correlation is not equal to 0</code></p>
<p>Comme pour les tests de comparaisons de moyennes, il est possible de réaliser un test unilatéral, à condition que cela ait un sens, à condition que nous soyons en mesure d’expliquer le choix de notre hypothèse alternative. La syntaxe est la même que pour les tests de Student ou de Wilcoxon : on utilise l’argument <code>alternative = "less"</code> ou <code>alternative = "greater"</code> au moment de faire le test, selon l’hypothèse que l’on souhaite tester.</p>
<p>Ici, si les hypothèses que nous souhaitons tester sont les suivantes :</p>
<ul>
<li>H<span class="math inline">\(_0\)</span> : le coefficient de corrélation entre les deux variables étudiées vaut 0 dans la population générale (<span class="math inline">\(\rho = 0\)</span>)</li>
<li>H<span class="math inline">\(_1\)</span> : le coefficient de corrélation entre les deux variables étudiées est positif dans la population générale (<span class="math inline">\(\rho &gt; 0\)</span>)</li>
</ul>
<p>On utilise la syntaxe suivante :</p>
<div class="sourceCode" id="cb171"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb171-1"><a href="séance-4-corrélations-et-régressions.html#cb171-1"></a><span class="kw">cor.test</span>(birds<span class="op">$</span>nVisitsNestling, birds<span class="op">$</span>futureBehavior,</span>
<span id="cb171-2"><a href="séance-4-corrélations-et-régressions.html#cb171-2"></a>         <span class="dt">alternative =</span> <span class="st">&quot;greater&quot;</span>)</span></code></pre></div>
<pre><code>
    Pearson&#39;s product-moment correlation

data:  birds$nVisitsNestling and birds$futureBehavior
t = 2.9603, df = 22, p-value = 0.003615
alternative hypothesis: true correlation is greater than 0
95 percent confidence interval:
 0.2320921 1.0000000
sample estimates:
      cor 
0.5337225 </code></pre>
<p>Comme pour les autres test unilatéraux, le choix d’une hypothèse alternative aberrante se traduit par une <span class="math inline">\(p-\)</span>value très forte, généralement égale à (ou très proche de) 1.</p>
</div>
<div id="estimation-et-intervalle-de-confiance" class="section level4">
<h4><span class="header-section-number">5.2.3.2</span> Estimation et intervalle de confiance</h4>
<p>Revenons à notre test bilatéral. La section “estimation” des résultats de ce test nous indique que la meilleure estimation du coefficient de corrélation linéaire de Pearson dans la population générale vaut <span class="math inline">\(\hat{\rho} = 0.533\)</span>. C’est la valeur que nous avions calculé à la main avec la fonction <code>cor()</code>.</p>
<p>L’intervalle de confiance à 95% de cette valeur estimée est également fourni. La conclusion de cette procédure pourrait donc être formulée de la façon suivante :</p>
<blockquote>
<p>Au seuil <span class="math inline">\(\alpha = 0.05\)</span>, le test de corrélation de Pearson a permis de rejeter l’hypothèse nulle selon laquelle le nombre de visites agressives au nid des poussins et leur futur comportement agressif sont indépendants (<span class="math inline">\(t = 2.96\)</span>, <span class="math inline">\(ddl = 22\)</span>, <span class="math inline">\(p = 0.007\)</span>). La meilleure estimation du coefficient de corrélation dans la population générale vaut <span class="math inline">\(\hat{\rho} = 0.533\)</span>. La vraie valeur dans la population générale a de bonnes chances de se trouver dans l’intervalle [0.17 ; 0.77] (intervalle de confiance à 95%).</p>
</blockquote>
</div>
<div id="conditions-dapplication-3" class="section level4">
<h4><span class="header-section-number">5.2.3.3</span> Conditions d’application</h4>
<p>Ce test est un test paramétrique. Pour avoir le droit de le réaliser, il nous faut donc vérifier les conditions d’application suivantes :</p>
<ol style="list-style-type: decimal">
<li>Les individus doivent être indépendants les uns des autres</li>
<li>Les mesures effectuées doivent suivre une <strong>distribution Normale bivariée</strong></li>
</ol>
<p>Sauf si on a de bonnes raisons de penser le contraire, on considère généralement que si l’échantillonnage a été fait de façon aléatoire, l’indépendance des observations est garantie. La condition de “distribution Normale bivariée” des données est en revanche nouvelle. Elle suppose essentiellement que les 3 critères suivants soient vérifiés :</p>
<ol style="list-style-type: decimal">
<li>La relation entre les 2 variables doit être linéaire. C’est que nous tentons de vérifier visuellement en réalisant un nuage de points des données.</li>
<li>Sur un graphique représentant une variable en fonction de l’autre, le nuage de points doit avoir une forme circulaire ou elliptique. Là encore, une représentation graphique nous permet d’apprécier cette condition.</li>
<li>Les 2 variables étudiées doivent suivre une distribution Normale dans la population générale. Avant de faire ce test, il nous faut donc vérifier la Normalité des données pour chacune des 2 variables séparément, à l’aide, par exemple, d’un test de Shapiro-Wilk.</li>
</ol>
<p>Pour résumer, l’examen du nuage de points permet de vérifier les 2 premières conditions et 2 tests de Shapiro permettent de vérifier la troisième. Pour l’examen du nuage de points, les conditions ne seront pas remplies dans les situations suivantes (voir les exemples du graphique ci-dessous) :</p>
<ul>
<li>Le nuage de points a une forme d’entonnoir ou de nœud papillon.</li>
<li>Des ouliers sont présents (quelques points fortement éloignés du reste des observations).</li>
<li>Une relation non linéaire existe entre les deux variables.</li>
</ul>
<p><img src="figure/unnamed-chunk-107-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>Enfin, si l’une, l’autre ou les deux séries de données ne suivent pas la loi Normale, il faudra faire un test non paramétrique.</p>
</div>
</div>
<div id="lalternative-non-paramétrique-4" class="section level3">
<h3><span class="header-section-number">5.2.4</span> L’alternative non paramétrique</h3>
<p>Quand les conditions d’application du test de corrélation de Pearson ne sont pas remplies, il faut faire un test équivalent non paramétrique. Le test utilisé le plus fréquemment dans cette situation est le test du <span class="math inline">\(\rho\)</span> de Spearman. On l’effectue comme le test de Pearson en précisant simplement un argument supplémentaire : <code>method = "spearman"</code> (sans maujscule) :</p>
<div class="sourceCode" id="cb173"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb173-1"><a href="séance-4-corrélations-et-régressions.html#cb173-1"></a><span class="kw">cor.test</span>(birds<span class="op">$</span>nVisitsNestling, birds<span class="op">$</span>futureBehavior,</span>
<span id="cb173-2"><a href="séance-4-corrélations-et-régressions.html#cb173-2"></a>         <span class="dt">method =</span> <span class="st">&quot;spearman&quot;</span>)</span></code></pre></div>
<pre><code>Warning in cor.test.default(birds$nVisitsNestling, birds$futureBehavior, :
Cannot compute exact p-value with ties</code></pre>
<pre><code>
    Spearman&#39;s rank correlation rho

data:  birds$nVisitsNestling and birds$futureBehavior
S = 1213.5, p-value = 0.01976
alternative hypothesis: true rho is not equal to 0
sample estimates:
     rho 
0.472374 </code></pre>
<p>Le test de Spearman est au test de Pearson ce que le test de Wilcoxon est au test de Student. Il travaille non pas sur les données brutes (ici, les mesures des scientifiques), mais sur des données modifiées, en l’occurence, sur les rangs des données. La première conséquence évidente est une perte de puissance notable par rapport au test de Pearson. Cette perte de puissance peut être ici observée par le biais de la <span class="math inline">\(p-\)</span>value plus élevée (donc moins significative) que pour le test précédent. Cela indique que même si la conclusion est la même, on rejette ici l’hypothèse nulle avec moins de confiance que pour le test de Pearson.</p>
<p>Le <span class="math inline">\(\rho\)</span> de Spearman est équivalent au <span class="math inline">\(r\)</span> de Pearson calculé sur les rangs des données. Lorsque plusieurs valeurs observées sont égales, plusieurs valeurs ont le même rang, ce qui cause l’apparition du message d’avertissement suivant :</p>
<p><code>Impossible de calculer la p-value exacte avec des ex-aequos</code></p>
<p>Ce message est sans conséquence tant que la <span class="math inline">\(p-\)</span>value du test de Spearman est éloignée du seuil <span class="math inline">\(\alpha\)</span> (ce qui est le cas ici). Mais quand <span class="math inline">\(p \approx \alpha\)</span>, il faut être particulièrement prudent quant à l’interprétation qui est faite des résultats.</p>
<p>Enfin, comme pour le test de Pearson, il est possible de réaliser un test de Spearman unilatéral en utilisant l’argument <code>alternative = "less"</code> ou <code>alternative = "greater"</code>. Les précautions à prendre pour utiliser ce genre de test sont toujours les mêmes.</p>
</div>
<div id="exercices" class="section level3">
<h3><span class="header-section-number">5.2.5</span> Exercices</h3>
<div id="canis-lupus" class="section level4">
<h4><span class="header-section-number">5.2.5.1</span> <em>Canis lupus</em></h4>
<p>En 1970, le loup <em>canis lupus</em> a été éradiqué en Norvège et en Suède. Autour de 1980, un couple de loups, originaire d’une population plus à l’Est, a fondé une nouvelle population en Suède. En l’espace de 20 ans, cette population comptait approximativement 100 loups. Il y a toutefois fort à craindre qu’une population fondée par un si petit nombre d’individus souffre de consanguinité. <span class="citation">Liberg et al. (<a href="#ref-liberg2005" role="doc-biblioref">2005</a>)</span> ont compilé les informations sur la reproduction dans cette population entre 1983 et 2002, et ils ont pu reconstruire le pédigrée des individus la composant. Ils ont ainsi été en mesure de déterminer avec précision le coefficient individuel de consanguinité dans 24 portées de louveteaux.
Pour mémoire, le coefficient individuel de consanguinité vaut 0 si ses parents ne sont pas apparentés, 0.25 si ses parents sont frères et sœurs issus de grands-parents non apparentés, et plus de 0.25 si les associations consanguines se répètent depuis plusieurs générations.</p>
<p>On souhaite déterminer si le coefficient de consanguinité est associé à la probabilité de survie des jeunes durant leur premier hiver. Les données de <span class="citation">Liberg et al. (<a href="#ref-liberg2005" role="doc-biblioref">2005</a>)</span> sont disponibles dans le fichier <a href="https://besibo.github.io/Biometrie3/data/loups.csv"><code>loups.csv</code></a>. La première colonne contient les coefficients de consanguinité et la seconde, le nombre de jeunes de chaque portée ayant survécu à leur premier hiver. Vous analyserez ces données en suivant l’ordre des étapes décrites plus haut. En particulier, vous prendrez soin de :</p>
<ul>
<li>Vérifier la qualité des données.</li>
<li>Mettre les données dans un format approprié si besoin.</li>
<li>Réaliser une exploration statistique puis visuelle des données.</li>
<li>Vérifier les conditions d’application d’un test paramétrique.</li>
<li>Faire le test approprié en posant les hypothèses nulles et alternatives judicieuses.</li>
<li>Répondre à la question posée en intégrant tous les éléments utiles.</li>
</ul>
</div>
<div id="les-miracles-de-la-mémoire" class="section level4">
<h4><span class="header-section-number">5.2.5.2</span> Les miracles de la mémoire</h4>
<p>À quel point les souvenirs d’évènements miraculeux sont-il fiables ? Une façon d’étudier cette question est de comparer différents récits de tours de magie extraordinaires. Parmis les tours les plus célèbres, on trouve celui de la corde du fakir. Dans l’une de ces versions, un magicien jette l’extrémité une corde d’apparence normale en l’air et cette corde devient rigide. Un garçon grimpe à la corde et finit par disparaître en haut de la scène. Le magicien lui demande de répondre mais n’obtient pas de réponse. Il attrape alors un couteau, grimpe à son tour, et le garçon, découpé en morceaux, tombe du ciel dans un panier posé par terre. Le magicien redescent de la corde et aide le garçon vivant, en un seul morceau et non blessé, à sortir du panier.</p>
<p><span class="citation">Wiseman &amp; Lamont (<a href="#ref-wiseman1996" role="doc-biblioref">1996</a>)</span> ont retrouvé 21 récits écrits de ce tour par des personnes ayant elles-mêmes assisté à ce tour. Ils ont attribué un score à chaque description selon le caractère plus ou moins impressionnant de la description. Par exemple, un score de 1 était attribué si le récit faisait état que “le garçon grimpe à la corde, puis il en redescend”. Les récits les plus impressionnants se sont vus attribuer la note de 5 (“le garçon grimpe, disparaît, est découpé en morceaux et réapparaît en chair et en os devant le public”).
Pour chaque récit, les chercheurs ont également enregistré le nombre d’années écoulées entre le moment où le témoin a assisté au tour de magie, et le moment où il a consigné son récit par écrit.</p>
<p>Y a-t’il un lien entre le caractère impressionnant (“<code>impressiveness</code>”) d’un souvenir et le temps écoulé jusqu’à l’écriture de sa description (“<code>years</code>”) ? Si oui, cela pourrait indiquer une tendance de la mémoire humaine à exagérer et à perdre en précision avec le temps.</p>
<p>Les données de <span class="citation">Wiseman &amp; Lamont (<a href="#ref-wiseman1996" role="doc-biblioref">1996</a>)</span> sont disponibles dans le fichier <a href="https://besibo.github.io/Biometrie3/data/ropetrick.csv"><code>ropetrick.csv</code></a>. Importez ces données et analysez-les en respectant les consignes de l’exercice précédent.</p>
</div>
</div>
</div>
<div id="régression-linéaire" class="section level2">
<h2><span class="header-section-number">5.3</span> Régression linéaire</h2>
<p>La régression est une méthode utilisée pour prédire les valeurs d’une variable numérique à partir des valeurs d’une seconde variable. Par exemple, le nuage de points de la figure ci-dessous montre comment la diversité génétique dans une population humaine locale peut être prédite par sa distance de dispersion depuis l’Est africain en ajustant une droite aux données <span class="citation">(d’après Whitlock &amp; Schluter, <a href="#ref-whitlock2015" role="doc-biblioref">2015</a>)</span>. L’homme moderne est apparu en Afrique et nos ancêtres ont perdu un peu de diversité génétique à chaque étape de leur colonisation de nouveaux territoires.</p>
<p><img src="images/genetics.jpg" /></p>
<p>Contrairement à la corrélation, ici, on n’examine pas seulement une éventuelle liaison : on suppose qu’une variable peut-être (en partie) expliquée par une autre. Nous aurons donc à distinguer les variables expliquées (ou dépendantes) qui figureront sur l’axe des y et seront nos variables prédites, et les variables explicatives (ou indépendantes) qui figureront sur l’axe des abscisses et seront les prédicteurs.</p>
<p>Une façon de distinguer corrélation et régression consiste à dire que “corrélation n’est pas causalité”. Si on compare un nombre de variables suffisamment important, on finira toujours par en trouver qui seront corrélées. Cela ne veut pas dire pour autant qu’il existe un lien de cause à effet entre l’une et l’autre. Pour vous en convaincre, examinez <a href="http://www.tylervigen.com/spurious-correlations">cette page</a>. Lorsque l’on s’intéresse à la régression linéaire, on essaie au contraire de prédire ou d’expliquer. En d’autres termes, on considère que les variations de la variable explicative sont au moins en partie la cause des variations de la variable expliquée.</p>
<p>Lorsque l’on s’intéresse à la régression linéaire, on considère que la relation qui lie les deux variables est linéaire, et on souhaite <strong>quantifier l’intensité de la relation</strong>. Nous allons examiner maintenant comment faire ça dans <code>R</code>.</p>
<div id="exploration-préalable-des-données-3" class="section level3">
<h3><span class="header-section-number">5.3.1</span> Exploration préalable des données</h3>
<p>Les activités humaines réduisent le nombre d’espèces dans un grand nombre d’écosystèmes à la surface du globe. Est-ce que cette diminution du nombre d’espèces affecte le fonctionnement de base des écosystèmes ? Où est-ce qu’au contraire, les espèces végétales sont majoritairement interchangeables, les fonctions des espèces disparues pouvants être assurées par les espèces toujours présentes ?</p>
<p>Pour tenter de répondre à cette question, <span class="citation">Tilman et al. (<a href="#ref-tilman2006" role="doc-biblioref">2006</a>)</span> ont ensemencé 161 parcelles de 9 mètres sur 9 mètres dans la réserve de Cedar Creek (Minesota, USA). Ils ont utilisé un nombre variable d’espèces typiques des prairies et ont mesuré la production de biomasse de chaque parcelle pendant 10 ans. Des lots de 1, 2, 4, 8 ou 16 plantes pluriannuelles (choisies au hasard parmi une liste de 18 espèces possibles) ont été assignés au hasard dans chacune des 161 parcelles. À l’issue des 10 années d’étude, les chercheurs ont mesuré un indice de stabilité de la biomasse en divisant la moyenne des biomasses sur 10 ans, par l’écart-type de ces mêmes biomasses.</p>
<p>Les données de cette expérience sont disponibles dans le fichier <a href="https://besibo.github.io/Biometrie3/data/plantbiomass.csv"><code>plantbiomass.csv</code></a>.</p>
<div id="importation-examen-visuel-et-statistiques-descriptives" class="section level4">
<h4><span class="header-section-number">5.3.1.1</span> Importation, examen visuel et statistiques descriptives</h4>
<p>Comme toujours, on importe les données et on commence par un examen visuel afin de détecter les éventuels problèmes et pour savoir où l’on va.</p>
<div class="sourceCode" id="cb176"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb176-1"><a href="séance-4-corrélations-et-régressions.html#cb176-1"></a>plant &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;data/plantbiomass.csv&quot;</span>)</span></code></pre></div>
<pre><code>Rows: 161 Columns: 2</code></pre>
<pre><code>── Column specification ────────────────────────────────────────────────────────
Delimiter: &quot;,&quot;
dbl (2): nSpecies, biomassStability</code></pre>
<pre><code>
ℹ Use `spec()` to retrieve the full column specification for this data.
ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.</code></pre>
<div class="sourceCode" id="cb180"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb180-1"><a href="séance-4-corrélations-et-régressions.html#cb180-1"></a>plant</span></code></pre></div>
<pre><code># A tibble: 161 × 2
   nSpecies biomassStability
      &lt;dbl&gt;            &lt;dbl&gt;
 1        1             7.47
 2        1             6.74
 3        1             6.61
 4        1             6.4 
 5        1             5.67
 6        1             5.26
 7        1             4.8 
 8        1             4.4 
 9        1             4.4 
10        1             4.26
# … with 151 more rows</code></pre>
<p>Ce premier examen nous montre que nous disposons bien de 161 observations pour 2 variables : le nombre d’espèces présentes dans la parcelle pendant 10 ans, et l’indice de stabilité de la biomasse de chaque parcelle.</p>
<div class="sourceCode" id="cb182"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb182-1"><a href="séance-4-corrélations-et-régressions.html#cb182-1"></a><span class="kw">skim</span>(plant)</span></code></pre></div>
<pre><code>── Data Summary ────────────────────────
                           Values
Name                       plant 
Number of rows             161   
Number of columns          2     
_______________________          
Column type frequency:           
  numeric                  2     
________________________         
Group variables            None  

── Variable type: numeric ──────────────────────────────────────────────────────
  skim_variable    n_missing complete_rate  mean    sd    p0   p25   p50   p75
1 nSpecies                 0             1  6.32  5.64  1     2        4   8  
2 biomassStability         0             1  4.42  1.95  1.34  3.07     4   5.2
   p100 hist 
1  16   ▇▁▂▁▃
2  15.8 ▇▆▁▁▁</code></pre>
<p>Ce premier examen nous montre que nous n’avons aucune données manquantes et que l’indice de stabilité a une distribution asymétrique (asymétrie droite) et qu’il varie d’un peu plus de 1 à près de 16. Pour en apprendre un peu plus, nous avons intérêt à examiner les données en groupes :</p>
<div class="sourceCode" id="cb184"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb184-1"><a href="séance-4-corrélations-et-régressions.html#cb184-1"></a>plant <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb184-2"><a href="séance-4-corrélations-et-régressions.html#cb184-2"></a><span class="st">  </span><span class="kw">group_by</span>(nSpecies) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb184-3"><a href="séance-4-corrélations-et-régressions.html#cb184-3"></a><span class="st">  </span><span class="kw">skim</span>()</span></code></pre></div>
<pre><code>── Data Summary ────────────────────────
                           Values    
Name                       Piped data
Number of rows             161       
Number of columns          2         
_______________________              
Column type frequency:               
  numeric                  1         
________________________             
Group variables            nSpecies  

── Variable type: numeric ──────────────────────────────────────────────────────
  skim_variable    nSpecies n_missing complete_rate  mean    sd    p0   p25
1 biomassStability        1         0             1  3.63  1.54  2.07  2.41
2 biomassStability        2         0             1  3.83  1.35  1.34  2.97
3 biomassStability        4         0             1  3.88  1.24  2.13  3   
4 biomassStability        8         0             1  4.62  1.19  2.53  3.75
5 biomassStability       16         0             1  6.03  2.73  2.93  4.03
    p50   p75  p100 hist 
1  3.01  4.4   7.47 ▇▂▂▁▂
2  3.73  4.51  7.41 ▃▇▆▂▁
3  3.86  4.54  7.41 ▇▆▆▂▁
4  4.40  5.03  7.74 ▃▇▃▃▁
5  5.27  7.07 15.8  ▇▅▂▁▁</code></pre>
<p>Cette fois, on obtient des informations pour chaque valeur de nombre d’espèce par parcelle. On constate par exemple que les moyennes de l’indice de stabilité de la biomasse augmentent très peu entre les catégories 1, 2 et 4 espèces par parcelle, mais que l’augmentation semble plus forte pour 8 et 16 espèces par parcelle. Tous les écarts-types semblent très proches, sauf peut-être pour la catégorie 16 espèces.</p>
<p>Une visualisation des données est toujours indispensable :</p>
<div class="sourceCode" id="cb186"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb186-1"><a href="séance-4-corrélations-et-régressions.html#cb186-1"></a>plant <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb186-2"><a href="séance-4-corrélations-et-régressions.html#cb186-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> nSpecies, <span class="dt">y =</span> biomassStability)) <span class="op">+</span></span>
<span id="cb186-3"><a href="séance-4-corrélations-et-régressions.html#cb186-3"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="fl">0.3</span>) <span class="op">+</span></span>
<span id="cb186-4"><a href="séance-4-corrélations-et-régressions.html#cb186-4"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Nombre d&#39;espèces par parcelle&quot;</span>,</span>
<span id="cb186-5"><a href="séance-4-corrélations-et-régressions.html#cb186-5"></a>       <span class="dt">y =</span> <span class="st">&quot;Indice de stabilité de la biomasse&quot;</span>) <span class="op">+</span></span>
<span id="cb186-6"><a href="séance-4-corrélations-et-régressions.html#cb186-6"></a><span class="st">  </span><span class="kw">theme_bw</span>()</span></code></pre></div>
<p><img src="figure/unnamed-chunk-114-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>Ce graphique nous apprend plusieurs choses :</p>
<ol style="list-style-type: decimal">
<li>Contrairement à la plupart des méthodes statistiques vues jusqu’ici, il n’est pas nécessaire que les données des variables soient distribuées selon une loi Normale. Ici, nous avons des données qui sont tout sauf normales pour la variable explicative puisque nous avons seulement les entiers 1, 2, 4, 8 et 16. Un histogramme ou une courbe de densité montre que la distribution de cette variable est très loin de la Normalité :</li>
</ol>
<div class="sourceCode" id="cb187"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb187-1"><a href="séance-4-corrélations-et-régressions.html#cb187-1"></a>plant <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb187-2"><a href="séance-4-corrélations-et-régressions.html#cb187-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> nSpecies)) <span class="op">+</span></span>
<span id="cb187-3"><a href="séance-4-corrélations-et-régressions.html#cb187-3"></a><span class="st">  </span><span class="kw">geom_density</span>(<span class="dt">fill =</span> <span class="st">&quot;steelblue&quot;</span>, <span class="dt">adjust =</span> <span class="fl">0.2</span>) <span class="op">+</span></span>
<span id="cb187-4"><a href="séance-4-corrélations-et-régressions.html#cb187-4"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Nombre d&#39;espèces par parcelle&quot;</span>) <span class="op">+</span></span>
<span id="cb187-5"><a href="séance-4-corrélations-et-régressions.html#cb187-5"></a><span class="st">  </span><span class="kw">theme_bw</span>()</span></code></pre></div>
<p><img src="figure/unnamed-chunk-115-1.png" width="90%" style="display: block; margin: auto;" />
Cela n’est pas du tout problématique : comme pour l’ANOVA, les conditions d’application porteront sur les <strong>résidus de la régression</strong>, pas sur les variables elles-mêmes.</p>
<ol start="2" style="list-style-type: decimal">
<li>Afin de limiter la sur-dispersion de la variable expliquée, notamment pour la catégorie <code>16</code> plantes par parcelle, nous allons transformer l’indice de stabilité en logarithme (attention, la fonction <code>log()</code> permet de calculer des logarithmes népériens ou logarithmes naturels) :</li>
</ol>
<div class="sourceCode" id="cb188"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb188-1"><a href="séance-4-corrélations-et-régressions.html#cb188-1"></a>plant &lt;-<span class="st"> </span>plant <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb188-2"><a href="séance-4-corrélations-et-régressions.html#cb188-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">log_biomass =</span> <span class="kw">log</span>(biomassStability))</span>
<span id="cb188-3"><a href="séance-4-corrélations-et-régressions.html#cb188-3"></a>plant</span></code></pre></div>
<pre><code># A tibble: 161 × 3
   nSpecies biomassStability log_biomass
      &lt;dbl&gt;            &lt;dbl&gt;       &lt;dbl&gt;
 1        1             7.47        2.01
 2        1             6.74        1.91
 3        1             6.61        1.89
 4        1             6.4         1.86
 5        1             5.67        1.74
 6        1             5.26        1.66
 7        1             4.8         1.57
 8        1             4.4         1.48
 9        1             4.4         1.48
10        1             4.26        1.45
# … with 151 more rows</code></pre>
<div class="sourceCode" id="cb190"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb190-1"><a href="séance-4-corrélations-et-régressions.html#cb190-1"></a>plant <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb190-2"><a href="séance-4-corrélations-et-régressions.html#cb190-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> nSpecies, <span class="dt">y =</span> log_biomass)) <span class="op">+</span></span>
<span id="cb190-3"><a href="séance-4-corrélations-et-régressions.html#cb190-3"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="fl">0.3</span>) <span class="op">+</span></span>
<span id="cb190-4"><a href="séance-4-corrélations-et-régressions.html#cb190-4"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Nombre d&#39;espèces par parcelle&quot;</span>,</span>
<span id="cb190-5"><a href="séance-4-corrélations-et-régressions.html#cb190-5"></a>       <span class="dt">y =</span> <span class="st">&quot;Transformation log</span><span class="ch">\n</span><span class="st"> de l&#39;indice de stabilité de la biomasse&quot;</span>) <span class="op">+</span></span>
<span id="cb190-6"><a href="séance-4-corrélations-et-régressions.html#cb190-6"></a><span class="st">  </span><span class="kw">theme_bw</span>()</span></code></pre></div>
<p><img src="figure/unnamed-chunk-117-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>On peut visualiser dès maintenant la droite de régression linéaire qui permet de lier ces deux variables grâce à la fonction <code>geom_smooth(method = "lm", se = FALSE</code>)` :</p>
<div class="sourceCode" id="cb191"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb191-1"><a href="séance-4-corrélations-et-régressions.html#cb191-1"></a>plant <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb191-2"><a href="séance-4-corrélations-et-régressions.html#cb191-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> nSpecies, <span class="dt">y =</span> log_biomass)) <span class="op">+</span></span>
<span id="cb191-3"><a href="séance-4-corrélations-et-régressions.html#cb191-3"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="fl">0.3</span>) <span class="op">+</span></span>
<span id="cb191-4"><a href="séance-4-corrélations-et-régressions.html#cb191-4"></a><span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>) <span class="op">+</span></span>
<span id="cb191-5"><a href="séance-4-corrélations-et-régressions.html#cb191-5"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Nombre d&#39;espèces par parcelle&quot;</span>,</span>
<span id="cb191-6"><a href="séance-4-corrélations-et-régressions.html#cb191-6"></a>       <span class="dt">y =</span> <span class="st">&quot;Transformation log</span><span class="ch">\n</span><span class="st"> de l&#39;indice de stabilité de la biomasse&quot;</span>) <span class="op">+</span></span>
<span id="cb191-7"><a href="séance-4-corrélations-et-régressions.html#cb191-7"></a><span class="st">  </span><span class="kw">theme_bw</span>()</span></code></pre></div>
<pre><code>`geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="figure/unnamed-chunk-118-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>À supposer que nous ayons le droit d’effectuer une régression linéaire (ce qu’il faudra vérifier avec les conditions d’application, <strong>après</strong> avoir fait la régression), la pente devrait être positive.</p>
</div>
</div>
<div id="le-test-paramétrique-5" class="section level3">
<h3><span class="header-section-number">5.3.2</span> Le test paramétrique</h3>
<p>À une exception près, la procédure de régression linéaire est en tous points identique à l’analyse de variance. Quand on fait une ANOVA, la variable expliquée est numérique et la variable explicative est catégorielle (c’est un facteur). Quand on fait une régression linéaire, les 2 variables sont numériques. Pour le reste, tout est identique : on exprime la variable expliquée en fonction de la variable explicative et on vérifie après coup, grâce aux résidus, si nous avions le droit ou non de faire l’analyse.</p>
<p>Pour faire une régression linéaire dans <code>R</code>, on utilise la fonction <code>lm()</code> (comme <strong>l</strong>inear <strong>m</strong>odel). Et comme pour l’ANOVA, les résultats de l’analyse doivent être stockés dans un objet puisque cet objet contiendra tous les éléments utiles pour vérifier les conditions d’application :</p>
<div class="sourceCode" id="cb193"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb193-1"><a href="séance-4-corrélations-et-régressions.html#cb193-1"></a>reg1 &lt;-<span class="st"> </span><span class="kw">lm</span>(log_biomass <span class="op">~</span><span class="st"> </span>nSpecies, <span class="dt">data =</span> plant)</span></code></pre></div>
<p>Faire une régression linéaire avec cette commande revient à effectuer en même temps 2 tests d’hypothèses indépendants : le premier concerne l’ordonnée à l’origine de la droite de régression et le second concerne la pente de la droite de régression. Les hypothèses de ces tests sont les suivantes :</p>
<p><em>Pour l’ordonnée à l’origine</em> (“intercept” en anglais) :</p>
<ul>
<li>H<span class="math inline">\(_0\)</span> : l’ordonnée à l’origine de la droite de régression vaut 0 dans la population générale.</li>
<li>H<span class="math inline">\(_1\)</span> : l’ordonnée à l’origine de la droite de régression est différente de 0 dans la population générale.</li>
</ul>
<p><em>Pour la pente</em> (“slope” en anglais) :</p>
<ul>
<li>H<span class="math inline">\(_0\)</span> : la pente de la droite de régression vaut 0 dans la population générale.</li>
<li>H<span class="math inline">\(_1\)</span> : la pente de la droite de régression est différente de 0 dans la population. générale</li>
</ul>
<div id="résultats-du-test-et-interprétation-1" class="section level4">
<h4><span class="header-section-number">5.3.2.1</span> Résultats du test et interprétation</h4>
<p>Comme pour l’ANOVA, on affiche les résultats de ces tests à l’aide de la fonction <code>summary()</code></p>
<div class="sourceCode" id="cb194"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb194-1"><a href="séance-4-corrélations-et-régressions.html#cb194-1"></a><span class="kw">summary</span>(reg1)</span></code></pre></div>
<pre><code>
Call:
lm(formula = log_biomass ~ nSpecies, data = plant)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.97148 -0.25984 -0.00234  0.23100  1.03237 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 1.198294   0.041298  29.016  &lt; 2e-16 ***
nSpecies    0.032926   0.004884   6.742 2.73e-10 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 0.3484 on 159 degrees of freedom
Multiple R-squared:  0.2223,    Adjusted R-squared:  0.2174 
F-statistic: 45.45 on 1 and 159 DF,  p-value: 2.733e-10</code></pre>
<p>Dans la forme, ces résultats sont très proches de ceux de l’ANOVA. La rubrique <code>Residuals</code> donne des informations sommaires sur les résidus. Ces informations sont utiles puisque les résidus serviront à vérifier les conditions d’application de la régression. À ce stade, on regarde surtout si la médiane des résidus est proche de 0 et si les résidus sont à peu près symétriques (les premier et troisième quartiles ont à peu près la même valeur absolue, idem pour le minimum et le maximum).</p>
<p>Le tableau <code>Coefficients</code> est celui qui nous intéresse le plus puisqu’il nous fournit, outre la réponse aux 2 tests, les estimations pour l’ordonnée à l’origine et la pente de la droite de régression. Ici, l’ordonnée à l’origine (intercept) est estimée à 1.198 (rappelez-vous que cette valeur fait référence au logarithme de la stabilité de la biomasse) et la pente à 0.033 (quand le nombre d’espèces augmente d’une unité, le logarithme de l’indice de stabilité de la biomasse augmente de 0.033 unités). Les <span class="math inline">\(p-\)</span>values de chacun des 2 tests sont fournies dans la dernière colonne et sont ici très inférieures à <span class="math inline">\(\alpha\)</span> : on rejette donc les 2 hypothèses nulles. En particulier, puisque l’hypothèse nulle est rejetée pour le test qui concerne la pente de la droite, on peut considérer que le nombre de plantes dans les parcelles influence bel et bien l’indice de stabilité de la biomasse. Autrement dit, le nombre de plantes dans les parcelles, permet, dans une certaine mesure, de prédire la valeur de l’indice de stabilité de la biomasse.</p>
<p>La relation n’est pas parfaite : le nombre de plantes dans chaque parcelle ne permet de prédire l’indice de stabilité de la biomasse que dans une mesure assez faible. C’est le <code>Adjusted R-squared</code> qui nous indique quelle est la “qualité” de prédiction du modèle. Ici, il vaut 0.22. Cela signifie que 22% des variations de l’indice de stabilité de la biomasse sont prédits par le nombre de plantes dans les parcelles. Une autre façon de présenter les choses consiste à dire que 78% des variations de l’indice de stabilité de biomasse sont expliqués par d’autres facteurs que le nombre d’espèces par parcelle. Le <span class="math inline">\(R^2\)</span> (à en pas confondre avec le coefficient de corrélation <span class="math inline">\(r\)</span>) renseigne donc sur la qualité de l’ajustement des données à la droite de régression. Il nous indique ici que le pouvoir prédictif de notre modèle linéaire est assez faible. Il est néanmoins significatif, ce qui indique que notre variable explicative joue bel et bien un rôle non négligeable dans les variations de la variable expliquée.</p>
</div>
<div id="intervalle-de-confiance-de-la-régression" class="section level4">
<h4><span class="header-section-number">5.3.2.2</span> Intervalle de confiance de la régression</h4>
<p>La pente et l’ordonnée à l’origine de la droite de régression ont été obtenues à partir des données d’un échantillon (ici, 161 parcelles). Il s’agit donc d’estimations des pentes et ordonnées à l’origine de la relation plus générale qui concerne la population globale. Comme toute estimation, les pentes et ordonnées à l’origine de la droite de régression sont donc entâchées d’incertitudes. Nous pouvons quantifier ces incertitudes grâce au calcul des intervalles de confiance à 95% de ces 2 paramètres :</p>
<div class="sourceCode" id="cb196"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb196-1"><a href="séance-4-corrélations-et-régressions.html#cb196-1"></a><span class="kw">confint</span>(reg1)</span></code></pre></div>
<pre><code>                 2.5 %     97.5 %
(Intercept) 1.11673087 1.27985782
nSpecies    0.02328063 0.04257117</code></pre>
<p>Ces résultats nous indiquent que les valeurs d’ordonnées à l’origine les plus probables dans la population générale sont vraisemblablement comprises entre 1.117 et 1.280. De même, les valeurs de pentes les plus probables dans la population générale sont vraisemblablement situées dans l’intervalle [0.023 ; 0.043].</p>
<p>Il est possible de visualiser cette incertitude grâce à la fonction <code>geom_smooth()</code> utilisée plus tôt :</p>
<div class="sourceCode" id="cb198"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb198-1"><a href="séance-4-corrélations-et-régressions.html#cb198-1"></a>plant <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb198-2"><a href="séance-4-corrélations-et-régressions.html#cb198-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> nSpecies, <span class="dt">y =</span> log_biomass)) <span class="op">+</span></span>
<span id="cb198-3"><a href="séance-4-corrélations-et-régressions.html#cb198-3"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="fl">0.3</span>) <span class="op">+</span></span>
<span id="cb198-4"><a href="séance-4-corrélations-et-régressions.html#cb198-4"></a><span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">TRUE</span>) <span class="op">+</span></span>
<span id="cb198-5"><a href="séance-4-corrélations-et-régressions.html#cb198-5"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Nombre d&#39;espèces par parcelle&quot;</span>,</span>
<span id="cb198-6"><a href="séance-4-corrélations-et-régressions.html#cb198-6"></a>       <span class="dt">y =</span> <span class="st">&quot;Transformation log</span><span class="ch">\n</span><span class="st"> de l&#39;indice de stabilité de la biomasse&quot;</span>) <span class="op">+</span></span>
<span id="cb198-7"><a href="séance-4-corrélations-et-régressions.html#cb198-7"></a><span class="st">  </span><span class="kw">theme_bw</span>()</span></code></pre></div>
<pre><code>`geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="figure/unnamed-chunk-122-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>Cet intervalle d’incertitude correspond à l’incertitude de la moyenne de la variable expliquée pour une valeur donnée de la variable explicative. Ainsi, par exemple, si le nombre d’espèces d’une parcelle est égal à 8, alors, la régression et son incertitude associée nous dit que le logarithme de l’indice de stabilité de la biomasse vaudra <strong>en moyenne</strong> environ 1.46, avec un intervalle de confiance de [1.40 ; 1.51]</p>
</div>
<div id="conditions-dapplication-4" class="section level4">
<h4><span class="header-section-number">5.3.2.3</span> Conditions d’application</h4>
<p>Les conditions d’application de la régression sont les mêmes que celles de l’ANOVA. Je vous renvoie donc au chapitre <a href="séance-3-comparer-la-moyenne-de-plus-de-2-groupes.html#CAANOVA">4.2.2.2</a> pour savoir quelles sont ces conditions d’application et comment les vérifier. J’insiste bien sur le fait que les conditions d’application sont absolument identiques à celles de l’ANOVA. Si je fais ici l’économie de la description, vous ne devez <strong>jamais faire l’économie</strong> de la vérification des conditions d’application.</p>
<div class="sourceCode" id="cb200"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb200-1"><a href="séance-4-corrélations-et-régressions.html#cb200-1"></a><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb200-2"><a href="séance-4-corrélations-et-régressions.html#cb200-2"></a><span class="kw">plot</span>(reg1)</span>
<span id="cb200-3"><a href="séance-4-corrélations-et-régressions.html#cb200-3"></a><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span></code></pre></div>
<p><img src="figure/unnamed-chunk-123-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>C’est seulement après avoir réalisé, examiné et commenté ces graphiques que vous serez en mesure de dire si oui ou non vous aviez le droit de faire la régression linéaire, et donc d’en interpréter les résultats.</p>
</div>
</div>
<div id="lalternative-non-paramétrique-5" class="section level3">
<h3><span class="header-section-number">5.3.3</span> L’alternative non paramétrique</h3>
<p>Lorsque les conditions d’application de la régression linéaire ne sont pas vérifiées, on a principalement deux options :</p>
<ol style="list-style-type: decimal">
<li>On essaie de transformer les données afin que les résidus de la régression se comportent mieux. Cela signifie tester différents types de transformations, ce qui peut être chronophage pour un résultat pas toujours garanti.</li>
<li>On utilise d’autre types de modèles de régression, en particulier les modèles de régressions linéaires généralisées (GLM), qui s’accommodent très bien de résidus non normaux et/ou non homogènes. Mais il s’agit là d’une toute autre classe de méthodes qui ne sont pas au programme de la licence.</li>
</ol>
</div>
<div id="exercices-1" class="section level3">
<h3><span class="header-section-number">5.3.4</span> Exercices</h3>
<div id="datasaurus-et-anscombe" class="section level4">
<h4><span class="header-section-number">5.3.4.1</span> Datasaurus et Anscombe</h4>
<p>Exécutez les commandes suivantes :</p>
<div class="sourceCode" id="cb201"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb201-1"><a href="séance-4-corrélations-et-régressions.html#cb201-1"></a><span class="kw">library</span>(datasauRus)</span>
<span id="cb201-2"><a href="séance-4-corrélations-et-régressions.html#cb201-2"></a></span>
<span id="cb201-3"><a href="séance-4-corrélations-et-régressions.html#cb201-3"></a>datasaurus_dozen <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb201-4"><a href="séance-4-corrélations-et-régressions.html#cb201-4"></a><span class="st">    </span><span class="kw">group_by</span>(dataset) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb201-5"><a href="séance-4-corrélations-et-régressions.html#cb201-5"></a><span class="st">    </span><span class="kw">summarize</span>(</span>
<span id="cb201-6"><a href="séance-4-corrélations-et-régressions.html#cb201-6"></a>      <span class="dt">moy_x    =</span> <span class="kw">mean</span>(x),</span>
<span id="cb201-7"><a href="séance-4-corrélations-et-régressions.html#cb201-7"></a>      <span class="dt">moy_y    =</span> <span class="kw">mean</span>(y),</span>
<span id="cb201-8"><a href="séance-4-corrélations-et-régressions.html#cb201-8"></a>      <span class="dt">ecart_type_x =</span> <span class="kw">sd</span>(x),</span>
<span id="cb201-9"><a href="séance-4-corrélations-et-régressions.html#cb201-9"></a>      <span class="dt">ecart_type_y =</span> <span class="kw">sd</span>(y),</span>
<span id="cb201-10"><a href="séance-4-corrélations-et-régressions.html#cb201-10"></a>      <span class="dt">correl_x_y  =</span> <span class="kw">cor</span>(x, y),</span>
<span id="cb201-11"><a href="séance-4-corrélations-et-régressions.html#cb201-11"></a>      <span class="dt">pente =</span> <span class="kw">coef</span>(<span class="kw">lm</span>(y<span class="op">~</span>x))[<span class="dv">2</span>],</span>
<span id="cb201-12"><a href="séance-4-corrélations-et-régressions.html#cb201-12"></a>      <span class="dt">ordonnee_origine =</span> <span class="kw">coef</span>(<span class="kw">lm</span>(y<span class="op">~</span>x))[<span class="dv">1</span>]</span>
<span id="cb201-13"><a href="séance-4-corrélations-et-régressions.html#cb201-13"></a>    )</span></code></pre></div>
<p>Examinez attentivement les nombreux résultats produits par cette commande. Vous devriez remarquer que pour ces 13 jeux de données, 2 variables numériques <code>x</code> et <code>y</code> sont mises en relation. Pour tous ces jeux de données, on observe que la moyenne de tous les <code>x</code> est la même, la moyenne de tous les <code>y</code> est la même, les écarts-types des <code>x</code> sont identiques, les écarts-types des <code>y</code> aussi, la corrélation entre <code>x</code> et <code>y</code> est également extrêmement proche pour tous les jeux de données, et lorsque l’on effectue une régression linéaire de <code>y</code> en fonction de <code>x</code>, les ordonnées à l’origine et les pentes des droites de régression sont extrêmement proches pour les 13 jeux de données.</p>
<p>Si on s’en tient à ces calculs d’indices synthétiques, on pourrait croire que ces jeux de données sont identiques ou presque. Pourtant, ce n’est pas par hasard que je vous répète à longueur de temps qu’il est <strong>indispensable de regarder les données</strong> avant de se lancer dans les analyses et les statistiques. Car ici, ces jeux de données sont très différents ! Conclure qu’ils sont identiques simplement parce que les statistiques descriptives sont égales, serait une erreur majeure :</p>
<div class="sourceCode" id="cb202"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb202-1"><a href="séance-4-corrélations-et-régressions.html#cb202-1"></a>datasaurus_dozen <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb202-2"><a href="séance-4-corrélations-et-régressions.html#cb202-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x, y, <span class="dt">color =</span> dataset)) <span class="op">+</span></span>
<span id="cb202-3"><a href="séance-4-corrélations-et-régressions.html#cb202-3"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">show.legend =</span> <span class="ot">FALSE</span>) <span class="op">+</span></span>
<span id="cb202-4"><a href="séance-4-corrélations-et-régressions.html#cb202-4"></a><span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>dataset, <span class="dt">ncol =</span> <span class="dv">3</span>) <span class="op">+</span></span>
<span id="cb202-5"><a href="séance-4-corrélations-et-régressions.html#cb202-5"></a><span class="st">  </span><span class="kw">theme_bw</span>()</span></code></pre></div>
<p><img src="figure/unnamed-chunk-125-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>Le quartet d’Anscombe est un autre exemple de ce type de problème.</p>
<p>Dans la console, exécutez la commande suivante (il vous faudra peut-être presser la touche Entrée plusieurs fois) pour produire les 4 graphiques d’Anscombe :</p>
<div class="sourceCode" id="cb203"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb203-1"><a href="séance-4-corrélations-et-régressions.html#cb203-1"></a><span class="kw">example</span>(anscombe)</span></code></pre></div>
<p>Examinez attentivement les nombreux résultats produits par cette commande dans la console, ainsi que les 4 graphiques obtenus. Vous devriez remarquer que pour ces 4 jeux de données, 2 variables numériques sont là encore mises en relation, et qu’elles présentent toutes les mêmes caractéristiques. En particulier, les régressions linéaires ont toutes les mêmes pentes et ordonnées à l’origine. Pourtant, seule l’une de ces régressions linéaires est valide. Pourquoi ?</p>
</div>
<div id="in-your-face" class="section level4">
<h4><span class="header-section-number">5.3.4.2</span> In your face</h4>
<p>Les hommes ont en moyenne un ratio “largeur du visage sur longueur du visage” supérieur à celui des femmes. Cela reflète des niveaux d’expression de la testostérone différents entre hommes et femmes au moment de la puberté. On sait aussi que les niveaux de testosterone permettent de prédire, dans une certaine mesure, l’agressivité chez les mâles de nombreuses espèces. On peut donc poser la question suivante : la forme du visage permet-elle de prédire l’agressivité ?</p>
<p>Pour tester cela, <span class="citation">Carré &amp; McCormick (<a href="#ref-carre2008" role="doc-biblioref">2008</a>)</span> ont suivi 21 joueurs de hockey sur glace au niveau universitaire. Ils ont tout d’abord mesuré le ratio largeur du visage sur longueur du visage de chaque sujet, puis, ils ont compté le nombre moyen de minutes de pénalité par match reçu par chaque sujet au cours de la saison, en se limitant aux pénalités infligées pour cause de brutalité. Les données sont fournies dans le fichier <a href="https://besibo.github.io/Biometrie3/data/hockey.csv"><code>hockey.csv</code></a>.</p>
<p>Importez, examinez et analysez ces données pour répondre à la question posée.</p>

</div>
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-carre2008">
<p><span class="smallcaps">Carré JM, &amp; McCormick CM</span>. (2008). In your face: Facial metrics predict aggressive behaviour in the laboratory and in varsity and professional hockey players. <em>Proceedings of the Royal Society B: Biological Sciences</em>, <strong>275</strong>(1651), 2651–2656. <a href="https://doi.org/10.1098/rspb.2008.0873">https://doi.org/10.1098/rspb.2008.0873</a></p>
</div>
<div id="ref-liberg2005">
<p><span class="smallcaps">Liberg O, Andrén H, Pedersen H-C, Sand H, Sejberg, Wabakken P, … Bensch S</span>. (2005). Severe inbreeding depression in a wild wolf Canis lupus population. <em>Biology Letters</em>, <strong>1</strong>(1), 17–20. <a href="https://doi.org/10.1098/rsbl.2004.0266">https://doi.org/10.1098/rsbl.2004.0266</a></p>
</div>
<div id="ref-muller2011">
<p><span class="smallcaps">Müller MS, Porter ET, Grace JK, Awkerman JA, Birchler KT, Gunderson AR, … Anderson DJ</span>. (2011). Maltreated nestlings exhibit correlated maltreatment as adults: Evidence of a “cycle of violence” in Nazca Boobies ( <em>Sula</em> <em>Granti</em> ). <em>The Auk</em>, <strong>128</strong>(4), 615–619. <a href="https://doi.org/10.1525/auk.2011.11008">https://doi.org/10.1525/auk.2011.11008</a></p>
</div>
<div id="ref-tilman2006">
<p><span class="smallcaps">Tilman D, Reich PB, &amp; Knops JMH</span>. (2006). Biodiversity and ecosystem stability in a decade-long grassland experiment. <em>Nature</em>, <strong>441</strong>(7093), 629. <a href="https://doi.org/10.1038/nature04742">https://doi.org/10.1038/nature04742</a></p>
</div>
<div id="ref-whitlock2015">
<p><span class="smallcaps">Whitlock M, &amp; Schluter D</span>. (2015). <em>The analysis of biological data</em> (Second edition). Greenwood Village, Colorado: Roberts and Company Publishers.</p>
</div>
<div id="ref-wiseman1996">
<p><span class="smallcaps">Wiseman R, &amp; Lamont P</span>. (1996). Unravelling the Indian rope-trick. <em>Nature</em>, <strong>383</strong>(6597), 212. <a href="https://doi.org/10.1038/383212a0">https://doi.org/10.1038/383212a0</a></p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="séance-3-comparer-la-moyenne-de-plus-de-2-groupes.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Biometrie3.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
